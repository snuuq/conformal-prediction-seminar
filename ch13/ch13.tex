\documentclass[compress]{beamer}
\usepackage{beamerthemeshadow}
\usetheme{snubeam}
\usepackage{snucode}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\hatP}{\hat{P}}
\newcommand{\eps}{\varepsilon}
\newcommand{\indicator}[1]{\mathds{1}\{#1\}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\Sn}{\mathcal{S}_n}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\dH}{d_{\mathrm{H}}}
\newcommand{\dTV}{d_{\mathrm{TV}}}

\begin{document}
\title{Ch 13. Conditional Independence Testing}
\author{Sungwoo Park}
\institute{
    Uncertainty Quantification Lab\\
    Seoul National University}
\date{January 12th, 2026}

{
    \begin{frame}[plain]
        \titlepage
    \end{frame}
}

\begin{frame}\frametitle{Table of contents}
    \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Conditional Independence Testing: Overview}
    \begin{block}{Problem Setting}
        Given i.i.d. samples $(X_i, Y_i, W_i)_{i \in [n]}$, test the null hypothesis:
        $$H_0: X \indep Y \mid W$$
        where $W$ is a \textbf{confounder} variable.
    \end{block}

    \begin{itemize}
        \item Is the feature $X$ associated with response $Y$ after accounting for $W$?
        \item Related to \textbf{variable selection} in high-dimensional regression
              \begin{itemize}
                  \item Testing $H_{0,j}: X_j \indep Y \mid X_{-j}$ for feature selection
              \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{Marginal vs. Conditional Independence}
    \begin{block}{Marginal Independence}
        $$H_0: X \indep Y$$
        \begin{itemize}
            \item No conditioning on any variable
            \item Does not account for confounding effects of $W$
            \item \textbf{Easy} to test via permutation tests : \emph{Theorem 13.1}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Marginal vs. Conditional Independence}
    \begin{block}{Conditional Independence}
        $$H_0: X \indep Y \mid W$$
        \begin{itemize}
            \item Must account for the confounder $W$
            \item \textbf{Easy} when $W$ is discrete (local permutation test) : \emph{Theorem 13.2}
            \item \textbf{Hard} (impossible) when $W$ is continuous (hardness result) : \emph{Theorem 13.3}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Q\&A: How is the Confounder $W$ Determined?}
    \begin{alertblock}{Question}
        How is the confounder $W$ typically determined in practice? It seems like finding an appropriate $W$ would be important, and trivial confounders might also exist.
    \end{alertblock}

    \begin{block}{Answer}
        \begin{itemize}
            \item This chapter \textbf{assumes $W$ is given} --- identifying $W$ is a separate problem
            \item In practice, $W$ selection is crucial:
                  \begin{itemize}
                      \item \textbf{Domain knowledge}: Prior understanding of causal relationships
                      \item \textbf{Variable selection}: In high-dimensional settings, $W = X_{-j}$ (all other variables)
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Q\&A: Why is Chapter 13 in This Book?}
    \begin{alertblock}{Question}
        Chapter 13 does not seem directly related to conformal prediction. Why is it included in this book?
    \end{alertblock}

    \begin{block}{Answer}
        \begin{itemize}
            \item This chapter demonstrates the broader applications of \textbf{permutation tests} and \textbf{exchangeability}
            \item The book's core techniques extend beyond conformal prediction:
                  \begin{itemize}
                      \item Chapter 2's permutation test framework
                      \item Chapter 4's test-conditional coverage hardness results
                      \item Chapter 11-12's binning approaches
                  \end{itemize}
            \item Emphasizes the \textbf{unifying theme} of ``distribution-free inference'' and shows how exchangeability-based arguments apply to hypothesis testing
        \end{itemize}
    \end{block}
\end{frame}

\section{Testing Marginal Independence}

\begin{frame}{Setup: Marginal Independence Testing}
    \begin{block}{Problem Setting}
        \begin{itemize}
            \item Data: $(X_1, Y_1), \ldots, (X_n, Y_n) \stackrel{\text{i.i.d.}}{\sim} P$
            \item Null hypothesis: $H_0: X \indep Y$
            \item Equivalently: $P \in \calP_{X \indep Y}$ (product distributions)
        \end{itemize}
    \end{block}

    \begin{block}{Test Statistic}
        Any function $T: (\calX \times \calY)^n \to \mathbb{R}$ measuring evidence against $H_0$.

        \textbf{Example (real-valued data):}
        $$T\big((X_1, Y_1), \ldots, (X_n, Y_n)\big) = \big|\text{Corr}\big((X_1, \ldots, X_n), (Y_1, \ldots, Y_n)\big)\big|$$
    \end{block}
\end{frame}

\begin{frame}{Permutation Test for Marginal Independence}
    \begin{block}{Key Insight}
        Under $H_0: X \indep Y$, permuting $X_i$'s while fixing $Y_i$'s doesn't change the distribution!
        $$(X_\sigma, \mathbf{Y}) \stackrel{d}{=} (\mathbf{X}, \mathbf{Y}) \quad \text{for all } \sigma \in \Sn$$
    \end{block}
    \begin{itemize}
        \item $\mathbf{X} = (X_1, \ldots, X_n)$, $\mathbf{Y} = (Y_1, \ldots, Y_n)$
        \item $X_\sigma = (X_{\sigma(1)}, \ldots, X_{\sigma(n)})$ for $\sigma \in \Sn$
    \end{itemize}
\end{frame}

\begin{frame}{Theorem 13.1: Validity of the Permutation Test}
    \begin{block}{Theorem 13.1}
        Fix any test statistic $T: (\calX \times \calY)^n \to \mathbb{R}$. Define the test:
        $$\psi(\mathbf{X}, \mathbf{Y}) = \indicator{\frac{1}{n!}\sum_{\sigma \in \Sn} \indicator{T(X_\sigma, \mathbf{Y}) \geq T(\mathbf{X}, \mathbf{Y})} \leq \alpha}$$

        Then $\psi$ is a \textbf{valid test} of $H_{X \indep Y}$:
        $$\mathbb{P}_P(\psi(\mathbf{X}, \mathbf{Y}) = 1) \leq \alpha \quad \text{for all } P \in \calP_{X \indep Y}$$
    \end{block}

    \begin{block}{Proof Idea}
        \begin{itemize}
            \item Under $H_0$: $P = P_X \times P_Y$
            \item Conditioning on $\mathbf{Y}$: $X_1, \ldots, X_n$ remain i.i.d. $\sim P_X \Rightarrow $ Checking $X_i$'s exchangeability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Recall: Permutation test for exchangeability}
    \begin{block}{Recall: Theorem 2.4}
        For any function $T: \mathcal{Z}^n \to \mathbb{R}$, the p-value $p$ defined as:
        $$p:=\frac{\sum_{\sigma \in S_n}\indicator{T(\mathbf{X}_\sigma) \geq T(\mathbf{X})}}{n!}$$

        satisfies $\mathbb{E}_P[p \leq \tau] \leq \tau$ for all $\tau \in [0,1]$ and $P \in \calP_{\text{exch}}$.
    \end{block}

    Plug-in $T \leftarrow T(\mathbf{X}, \mathbf{Y})$ into Theorem 2.4, conclued the proof.
\end{frame}

\begin{frame}{Q\&A: Alternative Test Statistics}
    \begin{alertblock}{Question}
        What are some alternative test statistics $T$ that can be used instead of the absolute Pearson correlation for testing marginal/conditional independence?
    \end{alertblock}

    \begin{block}{Answer: Alternative Test Statistics}
        By Theorem 2.4, we can use any test statistic $T$ to construct a valid permutation test. Here are some examples:
        \begin{enumerate}
            \item \textbf{Spearman's rank correlation} \cite{Spearman1904}: $|\text{Corr}(\text{rank}(X), \text{rank}(Y))|$
                  \begin{itemize}
                      \item rank is similar to order statistics' index
                      \item Robust to outliers, captures monotonic relationships
                  \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{Q\&A: Alternative Test Statistics}
    \begin{block}{Answer: Alternative Test Statistics}
        \begin{enumerate}
            \setcounter{enumi}{1}
            \item \textbf{Distance correlation} \cite{Szekely2007}
                  \begin{itemize}
                      \item Using distance covariance $\mathcal{V}^2(X, Y) := ||f_{X,Y}(t, s) - f_X(t)f_Y(s) ||^2$
                      \item Detects nonlinear dependencies, equals zero iff independent
                  \end{itemize}
            \item \textbf{HSIC} (Hilbert-Schmidt Independence Criterion) \cite{Gretton2005}
                  \begin{itemize}
                      \item $HSIC(p_{xy}, \mathcal{F}, \mathcal{G}) := ||C_{xy}||^2_{HS}=\mathbb{E}_{x,x',y,y'}\bigl[k(x,x')\,l(y,y')\bigr] + \mathbb{E}_{x,x'}\bigl[k(x,x')\bigr]\mathbb{E}_{y,y'}\bigl[l(y,y')\bigr] - 2\mathbb{E}_{x,y}\Bigl[\mathbb{E}_{x'}\bigl[k(x,x')\bigr]\mathbb{E}_{y'}\bigl[l(y,y')\bigr]\Bigr]$
                      \item RKHS-based measure of dependence
                  \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\section{Conditional Independence with Discrete $W$}

\begin{frame}{Local Permutation Test: Motivation}
    \begin{block}{Key Idea}
        When $W$ is discrete, group data points by their $W$ value:
        \begin{itemize}
            \item Within each group (same $W = w$): $X$ and $Y$ are \textbf{independent}
            \item Permute $X$ values \textbf{only within groups}
            \item This preserves the null distribution!
        \end{itemize}
    \end{block}

    \begin{block}{Intuition}
        \begin{itemize}
            \item Think of $W$ as defining ``subpopulations''
            \item Under $H_0: X \indep Y \mid W$, within each subpopulation $X$ and $Y$ are independent
            \item Same logic as marginal independence test, applied locally
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Local Permutation Test: Permutation}
    \begin{center}
        \begin{figure}
            \includegraphics[width = 5cm]{images/fig-1-local-permu.png}
        \end{figure}
    \end{center}
    \small Figure 1: Visualization of a dataset with a discrete confounder along with a local permutation of the covariate \cite{Angelopoulos2024-pf}
    \begin{block}{Allowed Permutations}
        Define the set of permutations that \textbf{preserve $W$ values}:
        $$\Sn(\mathbf{W}) = \{\sigma \in \Sn : W_{\sigma(i)} = W_i \text{ for all } i \in [n]\}$$
    \end{block}
\end{frame}

\begin{frame}{Local Permutation Test: Definition}
    \begin{block}{Test Definition (Equation 13.4)}
        \begin{align*}
             & \psi(\mathbf{X}, \mathbf{Y}, \mathbf{W})                                                                                                                                          \\
             & = \indicator{\frac{1}{|\Sn(\mathbf{W})|}\sum_{\sigma \in \Sn(\mathbf{W})} \indicator{T(X_\sigma, \mathbf{Y}, \mathbf{W}) \geq T(\mathbf{X}, \mathbf{Y}, \mathbf{W})} \leq \alpha}
        \end{align*}
    \end{block}

    \begin{itemize}
        \item Only permute $X$ values among data points with \textit{same} $W$ value
        \item This is called a ``\textbf{local}'' permutation test
    \end{itemize}
\end{frame}

\begin{frame}{Theorem 13.2: Validity of Local Permutation Test}
    \begin{block}{Theorem 13.2}
        Fix any test statistic $T: (\calX \times \calY \times \calW)^n \to \mathbb{R}$. Let $(X_i, Y_i, W_i)_{i \in [n]} \stackrel{\text{i.i.d.}}{\sim} P$. Then $\psi$ is a \textbf{valid test} of $H_{X \indep Y | W}$:
        $$\mathbb{P}_P(\psi(\mathbf{X}, \mathbf{Y}, \mathbf{W}) = 1) \leq \alpha \quad \text{for all } P \in \calP_{X \indep Y | W}$$
    \end{block}
    \begin{itemize}
        \item \textbf{Distribution-free}: No assumptions on $P$ (beyond conditional independence)
        \item \textbf{Test statistic-free}: Works for any choice of $T$
        \item Proof uses same techniques as Chapter 2 (permutation test validity)
    \end{itemize}
\end{frame}

\begin{frame}{When is the Local Permutation Test Powerful?}
    \begin{block}{Factor 1: Number of ``Clashes''}
        \begin{itemize}
            \item \textbf{Clashes}: pairs $i \neq j$ with $W_i = W_j$
            \item Many clashes $\Rightarrow$ rich set of permutations $\Rightarrow$ potential for power
            \item No clashes (all $W_i$ unique) $\Rightarrow$ $|\Sn(\mathbf{W})| = 1$ $\Rightarrow$ \textbf{no power}
        \end{itemize}
    \end{block}

    \begin{block}{Factor 2: Choice of Test Statistic $T$}
        \begin{itemize}
            \item $T$ must discriminate between null and alternative
            \item Standard considerations for hypothesis testing
        \end{itemize}
    \end{block}

    \begin{alertblock}{Critical Observation}
        If $W$ is \textbf{continuous}, all $W_i$'s are distinct a.s. $\Rightarrow$ \textbf{No power!}
    \end{alertblock}
\end{frame}

\begin{frame}{Proof Sketch: Theorem 13.2}
    \begin{enumerate}
        \item Conditional on $\mathbf{W}$ and $\mathbf{Y}$

              Under $H_0: X \indep Y \mid W$, conditioning on $(\mathbf{W}, \mathbf{Y})$:
              $$X_1, \ldots, X_n \text{ are independent (but not identically distributed)}$$
              with $X_i \sim P_{X|W}(\cdot | W_i)$.

        \item Exchangeability Within Groups

              For any $\sigma \in \Sn(\mathbf{W})$: since $W_{\sigma(i)} = W_i$,
              $$(X_\sigma, \mathbf{Y}) \mid \mathbf{W} \stackrel{d}{=} (\mathbf{X}, \mathbf{Y}) \mid \mathbf{W}\cdots (13.6)$$
    \end{enumerate}
\end{frame}

\begin{frame}{Proof Sketch: Theorem 13.2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item Using quantile technique

              We can check that $\Sn(\mathbf{w}) = \{\sigma \circ\sigma ' : \sigma ' \in \Sn(\mathbf{w})\}$ for any $\sigma \in \Sn(\mathbf{w})$. So,
              $$\hat{q}(\mathbf{x}_\sigma, \mathbf{y}, \mathbf{w}) = \text{Quantile}(T((\mathbf{x}_\sigma)_{\sigma'}, \mathbf{y}, \mathbf{w})_{\sigma' \in \Sn(\mathbf{w})}; 1- \alpha) = \hat{q}(\mathbf{x}, \mathbf{y}, \mathbf{w})$$

              Therefore,

              {\small
                      \begin{align*}
                           & \frac{1}{\lvert S_n(\mathbf{w}) \rvert} \sum_{\sigma \in S_n(\mathbf{w})} \mathbf{1}\{\psi(x_\sigma, y, \mathbf{w}) = 1\}                                                                                                                      \\
                           & = \frac{1}{\lvert S_n(\mathbf{w}) \rvert} \sum_{\sigma \in S_n(\mathbf{w})} \mathbf{1}\{T(x_\sigma, y, \mathbf{w}) > \hat{q}(x, y, \mathbf{w})\}                                                                                               \\
                           & = \frac{1}{\lvert S_n(\mathbf{w}) \rvert} \sum_{\sigma \in S_n(\mathbf{w})} \mathbf{1}\Bigl\{ T(x_\sigma, y, \mathbf{w}) > \operatorname{Quantile}\bigl((T(x_{\sigma'}, y, \mathbf{w}))_{\sigma' \in S_n(\mathbf{w})}; 1-\alpha \bigr) \Bigr\} \\
                           & \le \alpha
                      \end{align*}}
    \end{enumerate}
\end{frame}

\begin{frame}{Proof Sketch: Theorem 13.2}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item (13.6) implies that $$\mathbb{P}_{P}\bigl(\psi(\mathbf{X}, \mathbf{Y}, \mathbf{W}) = 1 \mid \mathbf{W}\bigr)
                  =
                  \mathbb{P}_{P}\bigl(\psi(\mathbf{X}_{\sigma}, \mathbf{Y}, \mathbf{W}) = 1 \mid \mathbf{W}\bigr)$$
              holds for each $\sigma \in \Sn(\mathbf{W})$. Taking an average over $\sigma \in \Sn(\mathbf{W})$,
              \begin{align*}
                   & \mathbb{P}_{P}\bigl(\psi(\mathbf{X}, \mathbf{Y}, \mathbf{W}) = 1 \mid \mathbf{W}\bigr) \\
                   & = \frac{1}{\lvert \Sn(\mathbf{W}) \rvert} \sum_{\sigma \in \Sn(\mathbf{W})}
                  \mathbb{P}_{P}\bigl(\psi(X_{\sigma}, \mathbf{Y}, \mathbf{W}) = 1 \mid \mathbf{W}\bigr)    \\
                   & = \mathbb{E}_{\mathbb{P}}\!\left[
                      \left.\frac{1}{\lvert \Sn(\mathbf{W}) \rvert}
                      \sum_{\sigma \in \Sn(\mathbf{W})}
                      \mathbf{1}\bigl\{\psi(X_{\sigma}, \mathbf{Y}, \mathbf{W}) = 1\bigr\}
                      \,\right|\, \mathbf{W}
                      \right] \leq \alpha
              \end{align*}
    \end{enumerate}
\end{frame}

\section{Hardness of Conditional Independence Testing}

\begin{frame}{The Problem with Continuous $W$}
    When $W$ has a \textbf{nonatomic} (continuous) distribution: $\mathbb{P}(W_i = W_j) = 0$ for $i \neq j$
    \begin{itemize}
        \item No two data points share the same $W$ value
        \item Local permutation test has no power
    \end{itemize}
\end{frame}

\begin{frame}{Theorem 13.3: Hardness Result}
    \begin{block}{Theorem 13.3}
        If the test $\psi$ has Type I error bounded by $\alpha$ for any distribution:
        $$\mathbb{P}_P(\psi(\mathbf{X}, \mathbf{Y}, \mathbf{W}) = 1) \leq \alpha \quad \text{for all } P \in \calP_{X \indep Y | W}$$

        Then the \textbf{power} of the test is not better than random for any $P$ with $P_W$ nonatomic:
        $$\mathbb{P}_P(\psi(\mathbf{X}, \mathbf{Y}, \mathbf{W}) = 1) \leq \alpha \text{ for \textbf{all} } P \in \calP_{(X ,Y , W)} \text{ with } P_W \text{ nonatomic}$$
    \end{block}

    \begin{alertblock}{Interpretation}
        For continuous $W$: \textbf{any} valid test has power $\leq \alpha$ (no better than random guessing!)
    \end{alertblock}
\end{frame}

\begin{frame}{Proof Sketch: Theorem 13.3}
    \begin{block}{Key Technique: Sample-Resample (Lemma 4.15)}
        Same technique as hardness results in Chapters 4, 11, 12!
    \end{block}

    \begin{enumerate}
        \item Fix arbitrary sequence $(X^{(i)}, Y^{(i)}, W^{(i)})_{i \in [M]}$ with distinct $W^{(i)}$
        \item Empirical distribution $\hatP_M = \frac{1}{M}\sum_{i=1}^M \delta_{(X^{(i)}, Y^{(i)}, W^{(i)})}$
        \item Under $\hatP_M$: $X, Y$ are deterministic functions of $W\Rightarrow$ Trivially $X \indep Y \mid W$ under $\hatP_M$!
        \item Valid test must have $\mathbb{P}_{\hatP_M}(\psi = 1) \leq \alpha$
        \item By Sample-Resample Method: this extends to any $P$ with nonatomic $P_W$
    \end{enumerate}
\end{frame}

\begin{frame}{Sample-Resample Method}
    \begin{block}{Recall: Lemma 4.15}
        Let $P$ be a distribution on $\mathcal{Z}$, and let $m, M \geq 1$. Let $Q$ denote the distribution on $\mathcal{Z}^m$ obtained by the following process to generate $(Z_1, \cdots, Z_m)$:
        \begin{enumerate}
            \item Sample $(Z^{(1)}, \cdots, Z^{(M)})$ i.i.d. from $P$, and define the empirical distribution $\hatP_M = \frac{1}{M}\sum_{i=1}^M \delta_{Z^{(i)}}$
            \item Resample $(Z_1, \cdots, Z_m)$ i.i.d. from $\hatP_M$
        \end{enumerate}
        Then
        $$d_{TV}(P^m, Q) \leq \frac{m(m-1)}{2M}$$
    \end{block}
\end{frame}

\begin{frame}{Proof (5) Sketch: Theorem 13.3}
    Construct $\mathbb{P}_{\hatP_M}$ by sampling the M data points i.i.d. from $P$ as Lemma 4.15, Sample $(X_1, Y_1, W_1), \cdots, (X_M, Y_M, W_M)$ i.i.d. from $\hatP_M$. So,
    $$\mathbb{P}_{Q}(\psi = 1) \leq \alpha \text{ a.s.}$$
    We also konw that $W^{(1)}, \cdots, W^{(M)}$ are distinct a.s.,
    $$\mathbb{P}_{\hatP_M}(\psi = 1 | \hatP_M) \leq \alpha \text{ a.s.}$$
    Using both inequality and Lemma 4.15,
    $$\mathbb{P}_{P}(\psi = 1) \leq \alpha + \frac{n(n-1)}{2M}$$
    and taking $M$ large to get desired result.

\end{frame}

\begin{frame}{Q\&A: Alternative Approaches Without Permutation Tests}
    \begin{alertblock}{Question}
        Are there methods to test conditional independence without using permutation tests?
    \end{alertblock}

    \begin{block}{Answer: Alternative Approaches (from Bibliographic Notes)}
        \begin{enumerate}
            \item \textbf{Conditional Chatterjee's correlation} \cite{Azadkia2021}
                  \begin{itemize}
                      \item Asymptotically valid, no smoothness assumptions needed
                  \end{itemize}
            \item \textbf{Model-X framework} \cite{Candes2018}
                  \begin{itemize}
                      \item Assumes knowledge of $P_{X|W}$
                      \item Resample $X$ under the null using known conditional distribution
                  \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{Q\&A: Alternative Approaches Without Permutation Tests}
    \begin{alertblock}{Question}
        Are there methods to test conditional independence without using permutation tests?
    \end{alertblock}

    \begin{block}{Answer: Alternative Approaches (from Bibliographic Notes)}
        \begin{enumerate}
            \setcounter{enumi}{2}
            \item \textbf{Knockoff filter} \cite{Barber2015}
                  \begin{itemize}
                      \item Uses ``pairwise exchangeability'' for FDR control
                  \end{itemize}
            \item \textbf{Generalized Covariance Measure} \cite{Shah2020}
                  \begin{itemize}
                      \item Uses residuals from regression of $X$ on $W$ and $Y$ on $W$
                  \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\section{Testing Under Smoothness Assumptions}

\begin{frame}{Escaping the Hardness Result}
    \begin{block}{The Dilemma}
        \begin{itemize}
            \item Distribution-free: Valid for \textit{all} $P \in \calP_{X \indep Y | W}$
            \item But: No power for continuous $W$!
        \end{itemize}
    \end{block}

    \begin{block}{Solution: Restrict the Null Hypothesis}
        \begin{itemize}
            \item Add a \textbf{smoothness assumption} on the conditional distribution
            \item Control Type I error against a \textit{smaller} class of nulls
            \item Allows nontrivial power while maintaining (approximate) validity
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Smoothness Assumption}
    \begin{block}{Definition: Hellinger Distance}
        For distributions $P_1, P_2$ with densities $f_1, f_2$:
        $$\dH(P_1, P_2) := \left(\frac{1}{2}\int_\calX \left(\sqrt{f_1(x)} - \sqrt{f_2(x)}\right)^2 d\mu(x)\right)^{1/2}$$
    \end{block}

    \begin{block}{Restricted Null}
        $$\calP^L_{X \indep Y | W} = \{P \in \calP_{X \indep Y | W} : L\text{-Lipschitz w.r.t. Hellinger distance}\}$$
    \end{block}

    \begin{block}{Lipschitz Condition (Equation 13.7)}
        Assume the map $w \mapsto P_{X|W}(\cdot | w)$ is $L$-Lipschitz w.r.t. Hellinger distance:
        $$\dH(P_{X|W}(\cdot | w), P_{X|W}(\cdot | w')) \leq L \|w - w'\|$$
        for all $w, w' \in \calW$.
    \end{block}
\end{frame}

\begin{frame}{Binned Local Permutation Test}
    \begin{block}{Key Idea}
        \begin{itemize}
            \item Partition $\calW$ into bins: $\calW = \cup_k \calW_k$
            \item Bin diameter bounded: $\max_k \sup_{w, w' \in \calW_k} \|w - w'\| \leq h$
        \end{itemize}
    \end{block}

    \begin{block}{Allowed Permutations}
        $\Sn^{\text{bin}}(\mathbf{W}) = \{\sigma \in \Sn : W_{\sigma(i)} \text{ and } W_i \text{ are in the same bin for all } i\}$
    \end{block}

    \begin{block}{Test Definition (Equation 13.8)}
        \begin{align*}
             & \psi(\mathbf{X}, \mathbf{Y}, \mathbf{W})                                                                                                                                                                     \\
             & := \indicator{\frac{1}{|\Sn^{\text{bin}}(\mathbf{W})|}\sum_{\sigma \in \Sn^{\text{bin}}(\mathbf{W})} \indicator{T(X_\sigma, \mathbf{Y}, \mathbf{W}) \geq T(\mathbf{X}, \mathbf{Y}, \mathbf{W})} \leq \alpha}
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}{Theorem 13.4: Validity of Binned Test}
    \begin{block}{Theorem 13.4}
        Fix any $T: (\calX \times \calY \times \calW)^n \to \mathbb{R}$. Let $(X_i, Y_i, W_i)_{i \in [n]} \stackrel{\text{i.i.d.}}{\sim} P$.

        For any partition with $\max_k \sup_{w, w' \in \calW_k} \|w - w'\| \leq h$:
        $$\mathbb{P}_P(\psi(\mathbf{X}, \mathbf{Y}, \mathbf{W}) = 1) \leq \alpha + Lh\sqrt{2n} \quad \text{for all } P \in \calP^L_{X \indep Y | W}$$
    \end{block}

    \begin{itemize}
        \item Type I error: $\alpha + Lh\sqrt{2n}$ (not exactly $\alpha$)
        \item Choose bin size $h = o(n^{-1/2})$ $\Rightarrow$ nearly valid test
        \item Trade-off: smaller $h$ = more valid, but fewer permutations = less power
    \end{itemize}
\end{frame}

\begin{frame}{Proof Sketch: Theorem 13.4}
    \begin{enumerate}
        \item Construct Auxiliary Distribution $\tilde{P}$
              \begin{itemize}
                  \item Express $P$ as mixture: $P = \sum_k \pi_k \cdot P^{(k)}$ where $\pi_k = \mathbb{P}(W \in \calW_k)$
                  \item Define $\tilde{P} = \sum_k \pi_k \cdot (P_X^{(k)} \times P_{(Y,W)}^{(k)})$
                  \item Under $\tilde{P}$: $X \indep (Y, W) \mid K$ where $K = k(W)$ (bin index)
              \end{itemize}

        \item $\tilde{P}$ Satisfies Conditional Independence
              \begin{itemize}
                  \item By construction: $X \indep (Y, W) \mid K$ under $\tilde{P}$
                  \item Local permutation test with discrete confounder $K$ is \textbf{exactly valid}
                  \item $\Rightarrow \mathbb{P}_{\tilde{P}}(\psi = 1) \leq \alpha$
              \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Q\&A: Hellinger Distance Convexity}
    \begin{alertblock}{Question}
        In the proof of Theorem 13.4, it mentions that $\dH(\cdot,\cdot)$ is convex in each argument. I believe $\dH^2(\cdot,\cdot)$ is convex, but $\dH(\cdot,\cdot)$ itself is not. Is my understanding correct?
    \end{alertblock}

    \begin{block}{Answer}
        \begin{itemize}
            \item $\dH(P, Q)$ is generally \textbf{not convex}
                  \begin{itemize}
                      \item Let $\begin{bmatrix}a\\b\end{bmatrix}$ denotes distribution s.t. $\mathbb{P}(\cdot = 0) = a$ and $\mathbb{P}(\cdot = 1) = b$
                      \item $\dH(\begin{bmatrix}\alpha\\1-\alpha\end{bmatrix}, \begin{bmatrix}1\\0\end{bmatrix})=\sqrt{1-\sqrt{\alpha}}$
                      \item $\alpha\dH(\begin{bmatrix}1\\0\end{bmatrix}, \begin{bmatrix}1\\0\end{bmatrix}) + (1-\alpha)\dH(\begin{bmatrix}0\\1\end{bmatrix}, \begin{bmatrix}1\\0\end{bmatrix})=1-\alpha$
                      \item But take $\alpha = \frac34$, $\sqrt{1-\sqrt{\alpha}} \not\leq 1-\alpha$
                  \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Q\&A: Hellinger Distance Convexity}
    \begin{block}{Answer}
        Just using $\dH^2$'s convexity:
        \begin{align*}
            \dH^2(P_{X|W}(\cdot|w), P_X^{(k)}) & = \dH^2(P_{X|W}(\cdot|w), \sum_{w' \in \mathcal{W}_k} \lambda_{w'} P_{X|W}(\cdot | w'))           \\
                                               & \leq \sum_{w' \in \mathcal{W}_k} \lambda_{w'} \cdot \dH^2(P_{X|W}(\cdot|w),  P_{X|W}(\cdot | w')) \\
                                               & \leq (Lh)^2
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}{Proof Sketch: Theorem 13.4 (continued)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item Bound Distance Between $P$ and $\tilde{P}$

              Using smoothness assumption and Hellinger distance properties:
              $$
                  \dH^2(P, \tilde{P}) = \mathbb{E}_P \left[ d_H^2(P_{X|W}(\cdot | W), P_X^{(k(W))}) \right] \leq (Lh)^2
              $$

        \item Extend to $n$ Samples
              \begin{itemize}
                  \item Subadditivity of squared Hellinger distance: $\dH^2(P^n, \tilde{P}^n) \leq n(Lh)^2$
                  \item Total variation bound: $\dTV(P^n, \tilde{P}^n) \leq \sqrt{2}\dH(P^n, \tilde{P}^n) \leq Lh\sqrt{2n}$
              \end{itemize}

              Therefore:
              $$\mathbb{P}_P(\psi = 1) \leq \mathbb{P}_{\tilde{P}}(\psi = 1) + Lh\sqrt{2n} \leq \alpha + Lh\sqrt{2n}$$
    \end{enumerate}
\end{frame}

\begin{frame}{Q\&A: Kernel Blurring for Continuous $W$?}
    \begin{alertblock}{Question}
        The local permutation test faces issues when $W$ is continuous, similar to Chapter 11. Can kernel-based blurring techniques from Chapter 11 be applied here?
    \end{alertblock}
    Under smoothness assumption, using Hellinger distance is similar to bulrring.
    \begin{itemize}
        \item Blurring : Making target $\mu_{P}(x)$ as smoothed/blurred version (weighted expectation):
              $$\hat{\mu}_P(x) = \frac{\mathbb{E}\left[\mu_P(X) \cdot H(x, X)\right]}{\mathbb{E}\left[H(x, X)\right]}$$
        \item Hellinger distance : Making conditional distribution $P_{X|W}(\cdot | w)$ as Binned weighted Expectation distribution:
              $$P_X^{(k)} = \int_{\mathcal{W}_k}P_{X|W}(\cdot | w')d\mu_k(w')$$
    \end{itemize}
\end{frame}

\section{Summary}

\begin{frame}{Connections to Other Chapters}
    \begin{table}
        \small
        \begin{tabular}{|l|c|c|}
            \hline
            \textbf{Topic}                       & \textbf{Discrete} & \textbf{Continuous} \\
            \hline
            Test-conditional coverage (Ch 4)     & Possible          & Hard                \\
            \hline
            Distribution-free regression (Ch 11) & Possible          & Hard                \\
            \hline
            ECE calibration (Ch 12)              & Estimable         & Not estimable       \\
            \hline
            Conditional indep. testing (Ch 13)   & Valid test        & No power            \\
            \hline
        \end{tabular}
    \end{table}

    \begin{block}{Unifying Theme}
        \begin{itemize}
            \item \textbf{Discrete setting}: Repeated observations enable inference
            \item \textbf{Continuous setting}: Fundamental impossibility results
            \item \textbf{Smoothness assumptions}: Bridge between the two
            \item \textbf{Binning}: Convert continuous to approximately discrete
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}\frametitle{}
    \vfill
    \begin{center}
        \begin{Huge}Thank you!\end{Huge}
    \end{center}
    \vfill
\end{frame}

\begin{frame}[t, allowframebreaks]\frametitle{References}
    \nocite{Angelopoulos2024-pf}
    \bibliography{ref}
\end{frame}

\end{document}

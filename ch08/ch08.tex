\documentclass[compress]{beamer}
\usepackage{beamerthemeshadow}
\usetheme{snubeam}
\usepackage{snucode}
\usepackage{dsfont}

\newcommand{\Dn}{\mathcal{D}_n}
\newcommand{\Dt}{\mathcal{D}_t}
\newcommand{\Ct}{\mathcal{C}_t}
\newcommand{\hatP}{\hat{P}}
\newcommand{\eps}{\varepsilon}
\newcommand{\indicator}[1]{\mathds{1}\{#1\}}

\begin{document}
\title{Ch 8. Online Conformal Prediction}
\author{Sungwoo Park}
\institute{
    Uncertainty Quantification Lab\\
    Seoul National University}
\date{November 17th, 2025}

{
% \usebackgroundtemplate{\includegraphics[width=\paperwidth]{images/snu-shift}}
\begin{frame}[plain]
  \titlepage
\end{frame}
}

\begin{frame}\frametitle{Table of contents}
    \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{From Batch to Online Setting}
\begin{block}{Batch Setting (Previous Chapters)}
\begin{itemize}
    \item Train on dataset $\Dn = \{(X_1, Y_1), \ldots, (X_n, Y_n)\}$
    \item Provide inference on future test points
    \item One-time prediction
\end{itemize}
\end{block}

\begin{block}{Online Setting (This Chapter)}
\begin{itemize}
    \item Observe data points \textbf{sequentially}
    \item At time $t$: observed $(X_1, Y_1), \ldots, (X_{t-1}, Y_{t-1})$ and $X_t$
    \item Construct prediction set $\Ct(X_t)$ for $Y_t$
    \item Add $(X_t, Y_t)$ to dataset and repeat
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Online Prediction Illustration}
\begin{center}
\begin{figure}
	\includegraphics[width = 10cm]{images/fig-1-online-settings.png}
\end{figure}
\end{center}
\small Figure 1: Online prediction loop — observe \(X_t\), produce \(\Ct(X_t)\), reveal \(Y_t\), and update the dataset. \cite{Angelopoulos2024-pf}
\end{frame}

\begin{frame}{New Challenges in Online Setting}
\begin{enumerate}
    \item \textbf{Data Reuse:} Each data point is used multiple times
    \begin{itemize}
        \item $(X_t, Y_t)$ is a test point at time $t$
        \item $(X_t, Y_t)$ becomes training data for times $t' > t$
    \end{itemize}
    
    \item \textbf{Multiple Prediction Sets:} Constructing many prediction sets over time
    \begin{itemize}
        \item Need coverage guarantees for each $C_t(X_t)$
        \item Need long-run average coverage guarantees
    \end{itemize}
    
    \item \textbf{Distribution Shift:} Distributions may change over time
    \begin{itemize}
        \item Exchangeability may not hold
        \item Need robust methods for non-stationary data
    \end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Q1: Why Study Sequences of Prediction Sets?}
\begin{alertblock}{Question}
\(X_1, X_2, X_3, \cdots, X_t\) 로부터 score function을 이용하여 \(C(X_t)\)을 구성하는 Conformal Prediction Flow에서 Online Comformal Prediction을 통한 추정구간의 열을 구하는 것이 실질적으로 의미가 있는 것인지 알고 싶습니다.
\end{alertblock}

\begin{block}{Answer}
    \begin{itemize}
        \item Online Data의 특성상 열로의 관찰이 필요함
        \item Corollary 8.3에서 제시된 수렴성을 확인할 수 있음
        \item Supermartingale의 특징을 활용해 이상치 관측 가능
    \end{itemize}
\end{block}
\end{frame}

\begin{frame}{Online Data Examples}
\begin{exampleblock}{Example 1: Medical ICU Monitoring}
\begin{itemize}
    \item Patient vital signs measured every hour
    \item Need prediction interval for next hour's blood pressure
    \item \textbf{Can't wait} to collect all 24 hours of data before making first prediction
    \item Need sequence: $C_1(X_1), C_2(X_2), \ldots, C_{24}(X_{24})$
    \item If coverage fails repeatedly, may indicate patient deterioration
\end{itemize}
\end{exampleblock}

\begin{exampleblock}{Example 2: Algorithmic Trading}
\begin{itemize}
    \item Stock price predictions every minute
    \item Trading decisions based on prediction intervals
    \item Need to detect when market regime changes (Section 8.2 test)
    \item Online conformal adapts to volatility changes (Section 8.3)
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Q2: Can We Use Split Conformal Online?}
\begin{alertblock}{Question}
Online conformal prediction은 Split conformal에는 적용할 수 없나요? 만약 적용할 수 있다면 어떤 방식으로 매 시점의 예측집합을 만드나요?
\end{alertblock}

\begin{block}{Answer: Maybe... NO}
Split Conformal에서는 Training set과 Calibration set이 나누어지는데, 이를 고정시켜 prediction을 하기에 연속성이 중요한 Online setting에서는 적합하지 않다고 생각합니다.
\end{block}
\end{frame}

\section{Online CP with Exchangeable Data}

\begin{frame}{Setup: Exchangeable Sequences}
\begin{block}{Assumptions}
\begin{itemize}
    \item Sequence $(X_1, Y_1), (X_2, Y_2), \ldots, (X_T, Y_T)$ is \textbf{exchangeable}
    \item At time $t$: observed $(X_1, Y_1), \ldots, (X_{t-1}, Y_{t-1})$ and $X_t$
    \item Goal: construct $\Ct(X_t)$ with coverage
    $$\mathbb{P}(Y_t \in \Ct(X_t)) \geq 1 - \alpha$$
\end{itemize}
\end{block}

\begin{block}{Algorithm}
Apply full conformal prediction (Algorithm 3.3) at each step:
\begin{itemize}
    \item Training data: $(X_1, Y_1), \ldots, (X_{t-1}, Y_{t-1})$
    \item Test point: $X_t$
    \item Score function: $s$ (symmetric)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Miscoverage Events}
\begin{block}{Definition}
Define the miscoverage indicator at time $t$:
$$\text{err}_t := \indicator{Y_t \notin \Ct(X_t)}$$
\end{block}

From Theorem 3.2(Marginal Coverage), we know:
$$\mathbb{E}[\text{err}_t] \leq \alpha \quad \text{for all } t$$

\begin{alertblock}{Question}
Does marginal coverage imply good \textbf{average coverage}?
$$\frac{1}{T}\sum_{t=1}^T \text{err}_t \approx \alpha \; ?$$
\end{alertblock}
\end{frame}

\begin{frame}{Online Prediction Guarantee}
\begin{block}{Proposition 8.1: Independence of Errors}
Suppose $(X_1, Y_1), (X_2, Y_2), \ldots, (X_T, Y_T)$ are exchangeable, the score function $s$ is symmetric, and the scores are distinct almost surely at each time $t$. Then the miscoverage events $\text{err}_t$ are \textbf{mutually independent}.
\end{block}

\begin{itemize}
    \item Each data point reused in constructing $C_{t'}$ for all $t' > t$
    \item Despite complex data reuse, errors are independent!
\end{itemize}

\end{frame}

\begin{frame}{Online Prediciton Guarantee : Conformal p-values}
\begin{block}{Definition (8.3)}
The conformal p-value at time $t$ is:
$$p_t = \frac{\sum_{i=1}^t \indicator{s((X_i, Y_i); \Dt) \geq s((X_t, Y_t); \Dt)}}{t}$$
where $\Dt = ((X_1, Y_1), \ldots, (X_t, Y_t))$.
\end{block}

\begin{block}{Theorem 8.2: Independence of Online Conformal p-values}
Under the same assumptions, $p_t$ is distributed as a discrete random variable on $\{1/t, 2/t, \ldots, 1\}$, and $p_1, \ldots, p_T$ are \textbf{mutually independent}.
\end{block}

We can easily check that $\text{err}_t = \indicator{p_t \leq \alpha}$ (Proposition 3.9), so Proposition 8.1 follows from Theorem 8.2.
\end{frame}

\begin{frame}{Proof Intuition (Theorem 8.2)}

\textbf{Key Idea:}
\begin{itemize}
    \item By symmetry of score function, $p_{t+1}$ does not depend on \textit{ordering} of $Z_1, \ldots, Z_t$
    \item $Z_1, \ldots, Z_t$ still exchangeable when conditioning on $p_{t+1}$
    \item But $p_t$ depends \textit{only} on ordering of first $t$ points
    \item This leads to independence!
\end{itemize}


\textbf{Proof Strategy:}
\begin{enumerate}
    \item Show: $p_t | (\hat{P}_t, Z_{t+1}, \ldots, Z_T) \sim \text{Unif}(\{1/t, \ldots, 1\})$
    \item Show: For $t' > t$, $p_{t'}$ is a function of $(\hat{P}_t, Z_{t+1}, \ldots, Z_T)$
    \item Combine: $p_t | (p_{t+1}, \ldots, p_T) \sim \text{Unif}(\{1/t, \ldots, 1\})$
    \item Therefore: $p_1, \ldots, p_T$ are independent
\end{enumerate}
where $\hat{P}_t = \frac{1}{t}\sum_{i=1}^t \delta_{Z_i}$ is empirical distribution.
\end{frame}

\begin{frame}{Average Coverage Guarantee}
\begin{block}{Corollary 8.3: Average Coverage of Online Conformal}
Suppose $(X_1, Y_1), (X_2, Y_2), \ldots$ are exchangeable, the score function is symmetric, and the scores are distinct almost surely at each time $t$. Then:
$$\frac{1}{T}\sum_{t=1}^T \indicator{Y_t \in C_t(X_t)} \to 1 - \alpha \text{ as } T \to \infty \text{ a.s.}$$

\end{block}

\begin{block}{Proof Sketch}
\begin{itemize}
    \item By Proposition 8.1: $\text{err}_1, \text{err}_2, \ldots$ are independent
    \item Each $\mathbb{E}[\text{err}_t] \in (\alpha - 1/t, \alpha]$ (minor variation)
    \item Apply Law of Large Numbers
    \item Can refine with Hoeffding's inequality for finite-sample bounds
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Without Distinct Scores Assumption}

If we don't assume distinct scores:
\begin{itemize}
    \item p-values $p_t$ may no longer be independent
    \item Conformal prediction becomes more conservative with ties
    \item Conservative version of Corollary 8.3 still holds
\end{itemize}

\begin{block}{Corollary 8.3 doesn't hold}
If $(X_1, Y_1), (X_2, Y_2), \ldots$ are exchangeable ONLY, then:
$$\liminf_{T \to \infty} \frac{1}{T}\sum_{t=1}^T \indicator{Y_t \in C_t(X_t)} \geq 1 - \alpha \text{ a.s.}$$
\end{block}
\end{frame}

\begin{frame}{Online Conformal Without Online Training?}
\begin{block}{Note}
Throughout Section 8.1, we assumed:
\begin{itemize}
    \item At time $t$: $(X_t, Y_t)$ is the test point
    \item At time $t+1$ (and beyond): $(X_t, Y_t)$ is part of training set
\end{itemize}
\end{block}

\begin{block}{Alternative Setting}
Sometimes not possible/desirable to add test data to training set:
\begin{itemize}
    \item Fix training set: $\{(X_i, Y_i)\}_{i \in [n]}$
    \item Test points $t = n+1, n+2, \ldots$ all compared to same training set
    \item Conformal p-values $p_{n+1}, p_{n+2}, \ldots$ will be \textbf{dependent}
    \item See Chapter 10.2 for analysis of this dependence
\end{itemize}
\end{block}
\end{frame}

\section{Testing Exchangeability Online}

\begin{frame}{Motivation: Detecting Distribution Shift}
\begin{block}{Why Test Exchangeability?}
\begin{itemize}
    \item Monitor online deployment of learning algorithms
    \item Identify presence of distribution shift
    \item Especially harmful shifts that affect algorithm errors
    \item Early detection of changepoints
\end{itemize}
\end{block}

\begin{block}{Main Idea}
\begin{itemize}
    \item Use conformal p-values from definition (8.3)
    \item Combine them into a \textbf{supermartingale}
    \item Under exchangeability: statistic stays small
    \item Large values = evidence against exchangeability
\end{itemize}
\end{block}
\end{frame}

\subsection{Supermartingale}
\begin{frame}{Supermartingales}
\begin{block}{Definition 8.4: Supermartingale}
A sequence $M_1, M_2, \ldots$ is a \textbf{supermartingale} if for all $t$:
\begin{enumerate}
    \item $\mathbb{E}[|M_t|] < \infty$
    \item $\mathbb{E}[M_t | M_1, \ldots, M_{t-1}] \leq M_{t-1}$ for all $t \geq 2$
\end{enumerate}
\end{block}

\vspace{0.3cm}

\begin{itemize}
    \item Sequence whose \textit{conditional expectation} is getting no larger over time
    \item If equality in (2): \textbf{martingale}
    \item Supermartingales tend to take small values
\end{itemize}
\end{frame}

\begin{frame}{Q3: Examples of Supermartingales}
\begin{alertblock}{Question}
Supermartingale의 예시를 몇 가지만 더 들어주실 수 있나요?
\end{alertblock}

\begin{exampleblock}{Example 1: Fair Game \cite{durrett}}
\begin{itemize}
    \item Gambler starts with wealth $M_0 = w$
    \item At each time $t$: bet \$1, win with prob $1/2$, lose with prob $1/2$
    \item Wealth: $M_t = M_{t-1} + X_t$ where $X_t \in \{-1, +1\}$ with equal probability
    \item This is a \textbf{martingale} : $\mathbb{E}[M_t | M_1, \ldots, M_{t-1}] = M_{t-1} + \mathbb{E}[X_t] = M_{t-1}$
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Q3: Examples of Supermartingales (cont.)}
\begin{exampleblock}{Example 2: Unfavorable Game \cite{durrett}}
\begin{itemize}
    \item Same setup, but now win with prob $p < 1/2$, lose with prob $1-p$
    \item $\mathbb{E}[M_t | M_1, \ldots, M_{t-1}] = M_{t-1} + p - (1-p) = M_{t-1} + (2p-1) < M_{t-1}$
    \item This is a \textbf{supermartingale} (unfavorable game)
    \item Wealth tends to decrease over time
\end{itemize}
\end{exampleblock}

\begin{exampleblock}{Example 3: Stopped Martingale \cite{durrett}}
\begin{itemize}
    \item Let $M_1, M_2, \ldots$ be a martingale
    \item Define stopping time $T$ (e.g., first time $M_t \geq$ threshold)
    \item Stopped process: $M_t^{T} = M_{\min(t, T)}$
    \item Then $M_t^{T}$ is a \textbf{supermartingale}
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Q3: Examples of Supermartingales (cont.)}
\begin{exampleblock}{Example 4: Non-negative Submartingale Reciprocal}
\begin{itemize}
    \item If $M_t$ is a non-negative submartingale: $\mathbb{E}[M_t | \mathcal{F}_{t-1}] \geq M_{t-1}$
    \item Then $N_t = 1/M_t$ is a supermartingale (by Jensen's inequality)
\end{itemize}
\end{exampleblock}

\begin{exampleblock}{Example 5: Product of Independent Superuniform RVs}
\begin{itemize}
    \item Let $U_1, U_2, \ldots$ be independent, superuniform (i.e., $\mathbb{P}(U_t \leq u) \leq u$)
    \item Define $M_t = \prod_{i=1}^t (1/U_i)$
    \item Then $M_t$ is a supermartingale
    \item \textbf{This is the type used in Proposition 8.5!}
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Q4: Understanding Supermartingales}
\begin{alertblock}{Question}
Supermartingale은 교환가능성이 성립할 때 작아진다는 것과, Supermartingale의 평균이 시간이 흐름에 따라 더 커지지 않는다는 게 이해가 잘 안 갑니다.
\end{alertblock}

\begin{block}{Answer}
    \begin{itemize}
        \item Exchangeability $\Rightarrow$ Making supermartingale (Prop 8.5)
        \item Supermartingale is defined on conditional expectation
    \end{itemize}
\end{block}
\end{frame}

\begin{frame}{Q4: Understanding Supermartingales (cont.)}
\begin{block}{Recall: Definition of Supermartingale}
A sequence $M_1, M_2, \ldots$ is a \textbf{supermartingale} if for all $t$:
\begin{enumerate}
    \item $\mathbb{E}[|M_t|] < \infty$
    \item $\mathbb{E}[M_t | M_1, \ldots, M_{t-1}] \leq M_{t-1}$ for all $t \geq 2$
\end{enumerate}
\end{block}

\textbf{Supermartingale does NOT mean:}
\begin{itemize}
    \item $M_t < M_{t-1}$ always (deterministically decreasing)
    \item $M_t$ cannot increase
\end{itemize}

\textbf{Supermartingale DOES mean:}
\begin{itemize}
    \item $\mathbb{E}[M_t | \text{past}] \leq M_{t-1}$ (expected to not increase)
    \item Individual realizations can go up or down
    \item But the "drift" is downward (or flat for martingales)
\end{itemize}
\end{frame}

\begin{frame}{Ville's Inequality}
\begin{block}{Ville's Inequality}
If $M_1, M_2, \ldots$ is a nonnegative supermartingale, then for any $a > 0$:
$$\mathbb{P}\left(\sup_{t \geq 1} M_t \geq a\right) \leq \frac{\mathbb{E}[M_1]}{a}$$
\end{block}

\vspace{0.3cm}

\begin{itemize}
    \item Stronger than Markov's inequality (which only applies at single time)
    \item Holds uniformly over all time
\end{itemize}
\end{frame}

\subsection{Construct super MG to test exchangeability}
\begin{frame}{A Simple Supermartingale from p-values}
\begin{block}{Proposition 8.5}
Consider conformal p-values $p_1, p_2, \ldots$ defined at (8.3). Let $\lambda \in [0,1]$ be fixed. If scores are distinct a.s. at each time $t$, then:
$$M_t = \prod_{t'=1}^t \frac{1 - \lambda p_{t'}}{1 - \lambda/2}$$
is a supermartingale.
\end{block}
\begin{block}{Proof Sketch}
\begin{itemize}
    \item $M_t = M_{t-1} \cdot \frac{1 - \lambda p_t}{1 - \lambda/2}$
    \item By Theorem 8.2: $p_t$ is superuniform and independent of $M_1, \ldots, M_{t-1}$
    \item Therefore: $\mathbb{E}[1 - \lambda p_t | M_1, \ldots, M_{t-1}] \leq 1 - \lambda/2$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{General Recipe for Supermartingales}
\begin{block}{Theorem 8.6: Online Test for Exchangeability}
Let $f_t: [0,1] \to [0, \infty)$ be nonincreasing functions with $\int_0^1 f_t(r) dr \leq 1$. Define:
$$M_t = \prod_{t'=1}^t f_{t'}(p_{t'})$$
Under exchangeability, symmetric score, and distinct scores:
\begin{enumerate}
    \item $M_t$ is a supermartingale
    \item For any $\alpha \in [0,1]$: $\mathbb{P}\left(\sup_t M_t \geq 1/\alpha\right) \leq \alpha$
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Proof Sketch (Theorem 8.6)}
\textbf{Part 1: $M_t$ is a supermartingale}
\begin{itemize}
    \item Using similar flow in Proposition 8.5:
    \item Since $f_t$ is nonincreasing: $\mathbb{E}[f_t(p_t)] \leq \int_0^1 f_t(r) dr \leq 1$
    \item Therefore: $\mathbb{E}[M_t | M_1, \ldots, M_{t-1}] = M_{t-1} \cdot \mathbb{E}[f_t(p_t)] \leq M_{t-1}$
\end{itemize}

\textbf{Part 2: Probability bound}
\begin{itemize}
    \item Apply Ville's inequality with $a = 1/\alpha$ and $\mathbb{E}[M_1] = \mathbb{E}[f_1(p_1)] \leq 1$
\end{itemize}
\end{frame}

\begin{frame}{Online Test Algorithm}
\begin{enumerate}
    \item Choose function \(f_t\) as specified in Theorem 8.6
    \begin{itemize}
            \item $f_t(r) = \frac{1-\lambda r}{1-\lambda/2}$ (Proposition 8.5)
            \item $f_t(r) = 2(1-r)$ (simple choice)
    \end{itemize}
    \item For each time \(t = 1, 2, \ldots\):
    \begin{enumerate}
        \item Observe \((X_t, Y_t)\) and compute conformal p-value and \(M_t\)
        \item If \(M_t \geq 1/\alpha\), reject exchangeability at significance level \(\alpha\). Otherwise, continue.
    \end{enumerate}
\end{enumerate}
\end{frame}

\begin{frame}{Visualization of Online Test}
\begin{center}
\begin{figure}
	\includegraphics[width = 10cm]{images/fig-2-online-exchange-test.png}
\end{figure}
\end{center}
\small Figure 2: The online test for exchangeability — Under null (exchangeability): crossing happens with probability \(\leq \alpha\) \cite{Angelopoulos2024-pf}
\end{frame}

\begin{frame}{Why is Conformal Prediction Critical?}
\begin{block}{Key Feature: No Multiplicity Penalty}
\begin{itemize}
    \item Testing exchangeability at \textit{every} time step $t = 1, 2, 3, \ldots$
    \item Reusing the same data over and over
    \item Naively: would need Bonferroni correction $\to$ no power
    \item \textbf{But}: Independence of conformal p-values (Theorem 8.2)!
    \item Can test at all times with \textit{single} $\alpha$ level
\end{itemize}
\end{block}

\begin{exampleblock}{Example Application}
\begin{itemize}
    \item Score: $s((x,y); \mathcal{D}) = y$
    \item p-value $p_t$ = rescaled rank of $Y_t$ relative to $Y_1, \ldots, Y_{t-1}$
    \item $M_t$ grows if we frequently observe small $p_t$
    \item Detects: values of $Y_t$ trending downward
    \item Application: changepoint detection in error rates
\end{itemize}
\end{exampleblock}
\end{frame}

\subsection{Conformal p-values and E-values}
\begin{frame}{Q4: Relationship to E-values}
\begin{alertblock}{Question}
현재 online CP의 setting과, proposition 8.5. (or Thm 8.6.) 이 제가 생각하기로는 E-value 와 상당히 유사해 보입니다. 둘 사이의 관계에 대해 자세한 설명 부탁드립니다.
\end{alertblock}

\begin{block}{Answer @ Bibliographic notes}
In particular, we note that the exchangeability supermartingale in Proposition 8.5 is a special case of an e-value, and these are useful more broadly. \cite{Ruf2023-dy}
\end{block}
\end{frame}

\begin{frame}{E-values: A Better Alternative of P-values}
\begin{block}{Definition: E-value (Evidence Value)}
A random variable $E \geq 0$ is an \textbf{e-value} for testing $H_0$ if:
$$\mathbb{E}_{H_0}[E] \leq 1$$
\end{block}

\textbf{Interpretation:}
\begin{itemize}
    \item Under null hypothesis, expected value $\leq 1$
    \item Large values of $E$ provide evidence against $H_0$
\end{itemize}

\begin{block}{Testing with E-values}
For any $\alpha \in (0,1)$, by Markov's inequality:
$$\mathbb{P}_{H_0}(E \geq 1/\alpha) \leq \frac{\mathbb{E}_{H_0}[E]}{1/\alpha} \leq \alpha$$

\textbf{Decision rule:} Reject $H_0$ if $E \geq 1/\alpha$
\end{block}
\end{frame}

\begin{frame}{E-values vs P-values}
\begin{table}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Property} & \textbf{P-value} & \textbf{E-value} \\
\hline
Range & $[0, 1]$ & $[0, \infty)$ \\
\hline
Under $H_0$ & Uniform $[0,1]$ & $\mathbb{E}[E] \leq 1$ \\
\hline
Evidence direction & Small = evidence & Large = evidence \\
\hline
Testing threshold & $p \leq \alpha$ & $E \geq 1/\alpha$ \\
\hline
Sequential testing & \textcolor{red}{Problematic} & \textcolor{green}{Natural} \\
\hline
Combination & \textcolor{orange}{Complex} & \textcolor{green}{Multiply} \\
\hline
\end{tabular}
\end{table}

\begin{exampleblock}{Converting between them}
If $p$ is a valid p-value, then $E = 1/p$ is \textbf{NOT} necessarily an e-value.\\
But if $p$ is \textit{superuniform}: $\mathbb{P}(p \leq u) \leq u$, then for nonincreasing $f$ with $\int_0^1 f(p) dp \leq 1$, we have $E = f(p)$ is an e-value. (proved \cite{Ruf2023-dy})
\end{exampleblock}
\end{frame}

\section{Prediction sets for adversarial sequences}
\begin{frame}{Beyond Exchangeability}
\begin{block}{Limitations of Exchangeability Assumption}
\begin{itemize}
    \item Real data often not exchangeable
    \item Distributions change over time (concept drift)
    \item Data might even be deterministic (not random)
\end{itemize}
\end{block}

\begin{block}{New Goal: Long-Run Coverage}
Without distributional assumptions, aim for:
$$\left|\frac{1}{T}\sum_{t=1}^T \text{err}_t - \alpha\right| \to 0$$
\begin{itemize}
    \item Average miscoverage rate $\approx \alpha$
    \item No assumption of exchangeability or even randomness!
    \item Works for \textbf{adversarial sequences}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Prediction Set Construction}
\begin{block}{General Form}
Prediction sets of the form:
$$\Ct(X_t) = \{y : s_t(X_t, y) \leq q_t\}$$
where:
\begin{itemize}
    \item $s_t$: score function (may depend on previous data)
    \item $q_t$: threshold (updated online)
\end{itemize}
\end{block}

\begin{block}{Connection to Conformal Prediction}
\begin{itemize}
    \item \textbf{Split conformal:} $s_t(x,y) = s(x,y)$ (fixed), $q_t = $ quantile
    \item \textbf{Full conformal:} $s_t(x,y) = s((x,y); \mathcal{D}_{t-1})$, $q_t = $ quantile
    \item \textbf{Adversarial setting:} Allow arbitrary $s_t$, update $q_t$ online
\end{itemize}
\end{block}
\end{frame}


\begin{frame}{Quantile Tracking Algorithm}
\begin{block}{Update Rule}
Initialize $q_1 \in [0, B]$ arbitrarily. For $t \geq 2$:
$$q_{t+1} = q_t + \eta_t(\text{err}_t - \alpha)$$
where $\eta_t > 0$ is the step size.
\end{block}

\begin{itemize}
    \item If $\text{err}_t = 1$ (miscoverage): \textit{increase} $q_{t+1}$ $\to$ more conservative
    \item If $\text{err}_t = 0$ (coverage): \textit{decrease} $q_{t+1}$ $\to$ less conservative
    \item Adaptive feedback mechanism
    \item No distributional assumptions needed!
\end{itemize}
\end{frame}

\begin{frame}{Q6: Quantile Tracking Algorithm as Optimization}
\begin{alertblock}{Question}
소단원 8.3에서 제시하는 quantile tracking 알고리즘의 update rule (식 8.8)을 보면 형태가 quantile loss에 대한 gradient descent (혹은 gradient ascent)와 닮아 보입니다. 혹시 식 8.8을 어떤 loss function의 최적화 알고리즘으로 생각할 수 있을까요?
\end{alertblock}

\begin{block}{Answer}
We can interpret the quantile tracking update as a form of stochastic gradient descent on the quantile loss function.
\end{block}
\end{frame}

\begin{frame}{Stochastic Greadient Descent Interpretation}
    \textbf{Quantile Loss:}
The $\alpha$-quantile can be found by minimizing:
$$L_\alpha(q) = \mathbb{E}[\rho_\alpha(Y - q)]$$
where $\rho_\alpha(u) = u(\alpha - \indicator{u < 0})$ is the "check function" or "pinball loss". Also, we can find \textbf{subgradient} as:

$$\partial_q \rho_\alpha(Y_t - q_t) = \begin{cases}
-\alpha & \text{if } Y_t > q_t \text{ (coverage, } \text{err}_t = 0) \\
1-\alpha & \text{if } Y_t < q_t \text{ (miscoverage, } \text{err}_t = 1)
\end{cases}$$
\end{frame}

\begin{frame}{Stochastic Greadient Descent Interpretation (cont.)}
\begin{block}{Connection to SGD}
The update rule $q_{t+1} = q_t + \eta_t(\text{err}_t - \alpha)$ is exactly:
$$q_{t+1} = q_t - \eta_t \cdot \partial_q \rho_\alpha(Y_t - q_t)$$
\end{block}

\textbf{This is stochastic gradient descent on quantile loss!}
\begin{itemize}
    \item Objective: track the $\alpha$-quantile of score distribution
    \item Update: use subgradient based on current observation
    \item Step size $\eta_t$: learning rate
\end{itemize}
\end{frame}

\begin{frame}{Main Result: Deterministic Coverage Guarantee}
\begin{block}{Theorem 8.7}
Consider an \textbf{arbitrary} sequence of data points $(X_1, Y_1), (X_2, Y_2), \ldots$ and \textbf{arbitrary} score functions $s_1, s_2, \ldots$ with $s_t(x,y) \in [0,B]$. Let step sizes $\eta_t$ be any nonincreasing positive sequence. Then for all $T \geq 1$:
$$\frac{1}{T}\sum_{t=1}^T \text{err}_t \in \left[\alpha \pm \frac{B + \eta_1}{\eta_T T}\right]$$
\end{block}

\begin{itemize}
    \item \textbf{Deterministic} guarantee (no probability!)
    \item For constant step size $\eta_t = \eta$: error concentrates at rate $O(1/T)$
    \item Holds for \textit{any} data sequence and \textit{any} choice of scores
    \item Works under distribution shift
\end{itemize}
\end{frame}

\begin{frame}{Proof Sketch (Theorem 8.7)}
\textbf{Part 1: The iterates are bounded.}

We can check $q_t \in [-\eta_1\alpha, B + \eta_1(1-\alpha)]$ by contradiction.

\textbf{Part 2: Simultaneous bounds on the weighted coverage gap.}
$$\left|\sum_{t=T_0}^{T_1} \eta_t(\text{err}_t - \alpha)\right| = |q_{T_1+1} - q_{T_0}| \leq B + \eta_1$$

\textbf{Part 3: Bounding the long-run coverage gap.}
\end{frame}

\begin{frame}{Visual intuition of boundness}
\begin{center}
\begin{figure}
	\includegraphics[width = 8cm]{images/fig-3-longrun-coverage.png}
\end{figure}
\end{center}
\small Figure 3: The long-run coverage gap — Once trajectory leaves thin envelope (where $q_T$ would be outside $[0,B]$), next prediction forces it back toward $\alpha$. \cite{Angelopoulos2024-pf}
\end{frame}


\begin{frame}{Choice of Step Sizes}
\begin{block}{Constant Step Size: $\eta_t = \eta$}
\textbf{Pros:}
\begin{itemize}
    \item Fast adaptation to changing score sequences
    \item Good for highly dynamic systems
\end{itemize}
\textbf{Cons:}
\begin{itemize}
    \item Quantiles $q_t$ fluctuate wildly
    \item Many infinite-size sets ($q_t > B$) and empty sets ($q_t < 0$)
    \item Never converges even in i.i.d. setting
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Choice of Step Sizes}
\begin{block}{Decaying Step Size: $\eta_t \propto t^{-(1/2+\epsilon)}$}
\textbf{Pros:}
\begin{itemize}
    \item Stabilizes quantile over time
    \item Can converge in stationary settings (by previous theorem)
\end{itemize}
\textbf{Cons:}
\begin{itemize}
    \item Slower adaptation: average error $\sim T^{-(1/2-\epsilon)}$ at time $T$
\end{itemize}
\end{block}
\end{frame}


\begin{frame}{I.I.D. Setting: Convergence Results}
\begin{block}{Theorem 8.8}
Suppose $(X_t, Y_t) \stackrel{\text{i.i.d.}}{\sim} P$ and scores $s_t$ trained online and $F_s$ is CDF of $s(X,Y)$ under $(X,Y) \sim P$.

\textbf{Constant step size} $\eta_t = \eta$:
$$\liminf_{t \to \infty} F_{s_t}(q_t) = 0, \quad \limsup_{t \to \infty} F_{s_t}(q_t) = 1$$
(infinitely many oscillations)

\textbf{Decaying step size} $\eta_t$ with $\sum_t \eta_t = \infty$, $\sum_t \eta_t^2 < \infty$:
$$\text{If } F_{s_t} \to F \text{ then } F_{s_t}(q_t) \to 1 - \alpha$$
(convergence to desired coverage)
\end{block}
\end{frame}

\begin{frame}\frametitle{}
    \vfill
    \begin{center}
        \begin{Huge}Thank you!\end{Huge}
    \end{center}
    \vfill
\end{frame}

\begin{frame}[t, allowframebreaks]\frametitle{References}
    % \nocite{*}  FIXME: this includes all references in ref.bib
    \bibliography{ref}
\end{frame}
\end{document}

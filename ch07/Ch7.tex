\documentclass{beamer}
\usefonttheme{professionalfonts}

% ===== engine: XeLaTeX 권장 =====
\usepackage{fontspec}   % XeLaTeX
\usepackage{xeCJK}      % CJK (Korean) support
% Beamer는 sans-serif가 기본이므로 둘 다 잡아주자
\setCJKmainfont{Noto Sans CJK KR}  % 본문 한글
\setCJKsansfont{Noto Sans CJK KR}  % 산세리프 한글

\usetheme{Madrid}
\usecolortheme{seahorse}

% 패키지
\usepackage{amsfonts}
\usepackage{hyperref}

\usepackage{cancel}
\usepackage{xcolor}

\renewcommand{\CancelColor}{\color{red}}

% 정보
\title{Ch.7 Weighted Variants of Conformal Prediction}
\subtitle{Conformal Prediction}
\author{Jikwang Kim}
\institute{Seoul National University}
\date{\today}

\AtBeginSection[]{
  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

% 표지
\begin{frame}
  \titlepage
\end{frame}

% 목차
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% 3 page
% 4 page
\section{Weighted quantiles and the weighted conformal algorithm}
\begin{frame}{Weighted Conformal Quantile}
  \begin{itemize}
    \item (Unweighted) Full CP
    \[
    \begin{aligned}
        &\mathcal{C}(X_{n+1}) = \{ y : S_{n+1}^y \leq \hat{q}^y \}, \\
        \text{where } \hat{q}^y =& \text{Quantile}(S_1^y, \ldots, S_n^y, S_{n+1}^y ; 1-\alpha) \\
        =& \text{Quantile}\left(\frac{1}{n+1} \sum_{i=1}^{n+1} \delta_{S_i^y} ; 1-\alpha\right)
    \end{aligned}
    \]
    \item Weighted Full CP
    \[
    \begin{aligned}
        &\mathcal{C}(X_{n+1}) = \{ y : S_{n+1}^y \leq \hat{q}_w^y \}, \\
        \text{where } &\hat{q}_w^y = \text{Quantile}\left(\sum_{i=1}^{n+1} w_i\delta_{S_i^y} ; 1-\alpha\right)
    \end{aligned}
    \]

    \item The weights can be fixed a-priori, or represent the ftn. of the data.
    
  \end{itemize}
\end{frame}

% 5 page
\begin{frame}{Weighted Conformal Algorithm}
  \begin{exampleblock}{\hypertarget{7.1}{Algorithm 7.1}: Weighted full CP}
    \begin{enumerate}
      \item Input: Training data $(X_1,Y_1), \ldots, (X_n,Y_n)$, test pt. $X_{n+1}$, target coverage level $1-\alpha$, conformal score ftn. $s$, and weight $w_1, \ldots w_{n+1}$
      \item For each possible response value $y \in \mathcal{Y},$ 다음을\ 계산
        \begin{enumerate}
            \item 각 sample 별 score $S_i^y = s((X_i, Y_i); \mathcal{D}_{n+1}^y), \quad i = 1, \ldots, n.$
            \item Test point score $S_{n+1}^y = s((X_{n+1}, y); \mathcal{D}_{n+1}^y)$
            \item Quantile \textcolor{red}{$\hat{q}_w^y = \text{Quantile}(\sum_{i=1}^{n+1} w_i\delta_{S_i^y} ; 1-\alpha)$}
        \end{enumerate}
      \item Return the prediction set $\mathcal{C}(X_{n+1}) = \{ y \in \mathcal{Y} : S_{n+1}^y \leq \hat{q}_w^y \}$
    \end{enumerate}
  \end{exampleblock}

  \textcolor{red}{WARNING!} The test point has not been replaced yet.
\end{frame}

% 6 page
\begin{frame}{Weighted Conformal Algorithm}
  \begin{exampleblock}{Algorithm 7.2: Weighted split CP}
    \begin{enumerate}
      \item Input: Pretraining data $\mathcal{D}_{\text{pre}}$, calibration data $(X_1,Y_1), \ldots, (X_n,Y_n)$, test pt. $X_{n+1}$, target coverage level $1-\alpha$, and weight $w_1, \ldots w_{n+1}$
      \item Score $s:\mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 학습, using $\mathcal{D}_{\text{pre}}$.
      \item $S_i = s(X_i, Y_i) \text{ for } i \in [n]$ 계산, and \newline
      \textcolor{red}{$\hat{q}_w = \text{Quantile}(\sum_{i=1}^{n} w_i\delta_{S_i^y} + w_{n+1}\delta_{+\infty} ; 1-\alpha)$} 계산
      \item Return the prediction set
      \[
      \mathcal{C}(X_{n+1}) = \{ y \in \mathcal{Y} : s(X_{n+1}, y) \leq \hat{q} \}
      \]
    \end{enumerate}
  \end{exampleblock}

  \textcolor{red}{WARNING!} The unreplaced test points are estimated conservatively.
\end{frame}

% 7 page
% 8 page
\section{Conformal prediction under covariate and label shifts}
\begin{frame}{Setup}
  \begin{itemize}
      \item Train data $(X_1, Y_1),\ldots,(X_n, Y_n) \sim P_{XY}$
      
      \vspace{0.5em}
      
      \item Test point $(X_{n+1}, Y_{n+1}) \sim \hat{P}_{XY}$

      \vspace{0.5em}
      
      \begin{itemize}
          \item Covariate-shifted : $(X_{n+1}, Y_{n+1}) \sim Q_X \times P_{Y \mid X}$
          \item[Ex)] 소득에\ 따른\ 보유\ 차량\ 종류 \\
          \; Training - 주로\ 서울($P_X$) / Test - 주로\ 부산($Q_X$) \\
          \; But 같은\ 소득에서는\ 같은\ 규칙 ($P_{Y \mid X}$)

          \vspace{0.5em}

          \item Label-shifted : $(X_{n+1}, Y_{n+1}) \sim P_{X \mid Y} \times Q_Y$
          \item[Ex)] (same) \\
          \; Training - 주로\ 소형차 ($P_Y$) / Test - 주로\ 준중형차 ($Q_Y$) \\
          \; But 각\ 클래스\ 내에서\ 동일한\ 규칙 ($P_{X \mid Y}$)
      \end{itemize}

      \vspace{0.5em}

      \item[$\Rightarrow$] $w_i = (\text{the likelihood ratio relating these two distributions})$
  \end{itemize}
  
  
\end{frame}

% 9 page
\begin{frame}{Covariate shift}
  \begin{itemize}
      \item Let $(X_1, Y_1), \ldots, (X_n, Y_n) \overset{\text{i.i.d.}}{\sim} P_X P_{Y|X}, \quad (X_{n+1}, Y_{n+1}) \sim Q_X P_{Y|X}.$
      \item If we know the LR for test versus train dist.s, we can be construct the weight:
      \[
      w_i \propto \frac{dQ_X}{dP_X}(X_i) \quad \text{where } \frac{dQ_X}{dP_X} \text{ is the Radon-Nykodym derivative.}
      \]
      \item Then, we can guarantee the marginal coverage.
  \end{itemize}
  
\end{frame}

% 10 page
\begin{frame}{Covariate shift}
  \begin{figure}
      \centering
      \includegraphics[width=0.5\linewidth]{image7.1.png}
      \label{fig:placeholder}
  \end{figure}
  \begin{itemize}
      \item For simplicity, let $s(X,Y) = I(X=1).$
      \item In above, $P_{P_X}(X=1) > P_{Q_X}(X=1)$ , i.e., $X=1$ is over-represented in the training data.
      \item[$\Rightarrow$] The unweighted conformal quantile will be biased downwards.

  \end{itemize}
  
\end{frame}

% 11 page
\begin{frame}{Cf. Radon-Nikodym Derivative}
    \begin{figure}
        \centering
        \includegraphics[width=0.85\linewidth]{image7.1.1.png}
        \caption{Extra explanation of RN derivative}
        \label{fig:placeholder}
    \end{figure}
\end{frame}

% 12 page
\begin{frame}{Cf. Radon-Nykodym Derivative}
    \textbf{(Most Important Thing) We have to know $P$, and $Q$!} i.e., \\
    {\small we should have confidence that the data is sampled in $P$, and test point is in $Q.$ \\
    Example)
    \begin{enumerate}
        \item Casual Inference \\
        For $(X_i, T_i, Y_i)=$ (covariate, treatment, outcome), assume the model is fitted by all observed data. Also, for $\tau^{\text{ATE}} = \mathbb{E}[Y_i(1) - Y_i(0)],$ and $\tau^{\text{ATT}} = \mathbb{E}[Y_i(1) - Y_i(0)|T_i=1],$ \\
        in ATE, $Q=P$, and in ATT, $Q_X=P_{X|T=1}.$
        \item Adaptive Learning
        \begin{itemize}
            \item $X$ : the protein sequence ($P_X= \text{Unif}(X;\mathcal{X})$), and $Y$ : the fitness of protein
            \item fit the regression model $\mu_{Z_{1:n}}$, and propose the sequence with the highest predicted fitness ($Q_X=\tilde{P}_{X;Z_{1:n}}=\exp(\lambda \cdot \mu_{Z_{1:n}}(X_{\text{test}}))$)
            \begin{figure}
                \centering
                \includegraphics[width=0.5\linewidth]{image7.1.2.png}
                \label{fig:placeholder}
            \end{figure}
        \end{itemize}
    \end{enumerate}}
\end{frame}

% 13 page
\begin{frame}{Covariate shift}
    Main Issue : Non-exchangeability for train and test.
    \begin{itemize}
        \begin{alertblock}{{\small (Recall) Prop 2.2: Conditioning on the empirical distribution}}
        \[
        Z_i \text{'s are exchangeable} \quad \Longleftrightarrow \quad  Z_i|\hat{P}_n \sim \hat{P}_n
        \]
        \end{alertblock}

        \item Now, $(X_{n+1}, Y_{n+1})|\hat{P}_{n+1} \not\sim \hat{P}_{n+1}$
        \item Instead,
        \[
        (X_{n+1}, Y_{n+1})|\hat{P}_{n+1} \sim \sum_{i=1}^{n+1} \frac{dQ_X}{dP_X}(X_i) \cdot \delta_{(X_i,Y_i)}
        \]
        \item Hence, $S_{n+1} \,\big|\, \hat{P}_{n+1} \sim \sum_{i=1}^{n+1} w_i \delta_{S_i}$
    \end{itemize}
\end{frame}

% 14 page
\begin{frame}{Covariate shift}
  
  \begin{alertblock}{Thm 7.3: Weighted CP with Covariate shift}
    {\small For $(X_1, Y_1), \ldots,(X_{n}, Y_{n}) \overset{\text{i.i.d.}}{\sim} P_X \times P_{Y \mid X}$, and $(X_{n+1}, Y_{n+1}) \sim Q_X \times P_{Y \mid X}$ independently, let $Q_X \ll P_X$. (i.e., $\frac{dQ_X}{dP_X}(x) < \infty \; \forall x \in \mathcal{X}$) \\
    Fix any symmetric score $s$, and define the prediction set
    \[
    \mathcal{C}(X_{n+1}) = \{ y : S_{n+1}^y \leq \hat{q}_w^y \}, \quad \text{where } \hat{q}_w^y = \text{Quantile}\left(\sum_{i=1}^{{\color{red} n}} w_i\delta_{S_i^y} ; 1-\alpha\right)
    \]
    where
    \[
    w_i = \frac{ \frac{dQ_X}{dP_X}(X_i) }{ \sum_{j=1}^{n+1} \frac{dQ_X}{dP_X}(X_j) }
    \]
    Then, it satisfies the marginal coverage guarantee,
    \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \geq 1 - \alpha
    \]}
  \end{alertblock}

  \textcolor{red}{WARNING!} $w_{n+1}\delta_{\infty}$ can be omited.
  
\end{frame}

% 15 page
\begin{frame}{Label shift}
  \begin{itemize}
      \item Now, let $(X_1, Y_1), \ldots, (X_n, Y_n) \overset{\text{i.i.d.}}{\sim} P_{X\mid Y} P_{Y}, \quad (X_{n+1}, Y_{n+1}) \sim P_{X|Y}Q_Y.$
      \item Similarly,
      \[
      w_i \propto \frac{dQ_Y}{dP_Y}(Y_i) \quad \text{where } \frac{dQ_Y}{dP_Y} \text{ is the Radon-Nykodym derivative.}
      \]

      \vspace{1.0em}
      \item However, there are still some problems remaining: 
      \item[] the test point $Y_{n+1}$ is not fixed in CP.
      \item[$\Rightarrow$] The weights depend on the hypothesized test point $y$.
  \end{itemize}
\end{frame}

% 16 page
\begin{frame}{Label shift}
  \begin{alertblock}{Thm 7.4: Weighted CP with Label shift}
    {\small For $(X_1, Y_1), \ldots,(X_{n}, Y_{n}) \overset{\text{i.i.d.}}{\sim} P_{X\mid Y}\times P_{Y}$, and $(X_{n+1}, Y_{n+1}) \sim P_{X|Y}\times Q_Y$ independently, let $Q_Y \ll P_Y$. (i.e., $\frac{dQ_Y}{dP_Y}(y) < \infty \; \forall y \in \mathcal{Y}$) \\
    Fix any symmetric score $s$, and define the prediction set
    \[
    \mathcal{C}(X_{n+1}) = \{ y : S_{n+1}^y \leq \hat{q}_w^y \}, \quad \text{where } \hat{q}_w^y = \text{Quantile}\left(\sum_{i=1}^{n} {\color{red} w_i^y}\delta_{S_i^y} ; 1-\alpha\right)
    \]
    where, for $i \in [n]$,
    \[
    w_i^y = \frac{\frac{dQ_Y}{dP_Y}(Y_i)} {\sum_{j=1}^n \frac{dQ_Y}{dP_Y}(Y_j) + \frac{dQ_Y}{dP_Y}(y)},  
    \quad \text{and } w_{n+1}^y = \frac{\frac{dQ_Y}{dP_Y}(y)}{\sum_{j=1}^n \frac{dQ_Y}{dP_Y}(Y_j) + \frac{dQ_Y}{dP_Y}(y)}
    \]
    Then, it satisfies the marginal coverage guarantee,
    \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \geq 1 - \alpha
    \]}
  \end{alertblock}
\end{frame}


% 17 page
\begin{frame}{General shift between the training dist. and test dist.}
  \begin{alertblock}{Thm 7.5: Weighted CP with General shift}
    {\small For $(X_1, Y_1), \ldots,(X_{n}, Y_{n}) \overset{\text{i.i.d.}}{\sim} P$, and $(X_{n+1}, Y_{n+1}) \sim Q$ independently, let $Q \ll P$. (i.e., $\frac{dQ}{dP}(y) < \infty \; \forall y \in \mathcal{Y}$) \\
    Fix any symmetric score $s$, and define the prediction set
    \[
    \mathcal{C}(X_{n+1}) = \{ y : S_{n+1}^y \leq \hat{q}_w^y \}, \quad \text{where } \hat{q}_w^y = \text{Quantile}\left(\sum_{i=1}^{n} w_i^y\delta_{S_i^y} ; 1-\alpha\right)
    \]
    where, for $i \in [n]$,
    {\tiny \[
    w_i^y = \frac{\frac{dQ}{dP}(X_i, Y_i)} {\sum_{j=1}^n \frac{dQ}{dP}(X_j, Y_j) + \frac{dQ}{dP}(X_{n+1}, y)},  
    \quad \text{and } w_{n+1}^y = \frac{\frac{dQ}{dP}(X_{n+1}, y)}{\sum_{j=1}^n \frac{dQ}{dP}(X_j, Y_j) + \frac{dQ}{dP}(X_{n+1}, y)} \tag{7.3}
    \]}
    Then, it satisfies the marginal coverage guarantee,
    \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \geq 1 - \alpha
    \]}
  \end{alertblock}
\end{frame}

% 18 page
\begin{frame}{Marginal coverage guarantee with General shift}
  \begin{alertblock}{Proposition 7.6: Conditioning on the empirical distribution}
    Let $Z_1, \ldots, Z_n \overset{\text{i.i.d.}}{\sim} P$ and $Z_{n+1} \sim Q$ independently, for some dist.s $P, Q$ on $\mathcal{Z}.$ Also, $Q \ll P$. (i.e., $\frac{dQ}{dP}(z) < \infty \; \forall z \in \mathcal{Z}$) \\
    Let the empirical dist. of the $n+1$ data points be
    \[
    \hat{P}_{n+1} = \frac{1}{n+1} \sum_{i=1}^{n+1}\delta_{Z_i}
    \]

    Then,
    \[
    Z_{n+1}\mid \hat{P}_{n+1} \sim \sum_{i=1}^{n+1} w_i \delta_{Z_i}
    \]
    where 
    \[
    w_i = \frac{\frac{dQ}{dP}(Z_i)} {\sum_{j=1}^{n+1} \frac{dQ}{dP}(Z_j)}, \quad i \in [n+1]
    \]
  \end{alertblock}
\end{frame}

% 19 page
\begin{frame}{Marginal coverage guarantee with General shift}
  proof of Thm 7.5) Note that
  {\footnotesize \[
  Y_{n+1} \in \mathcal{C}(X_{n+1}) \;\;\Longleftrightarrow\;\; S_{n+1} \leq \hat{q}_w^{Y_{n+1}} \;\;\Longleftrightarrow\;\; S_{n+1} \leq \text{Quantile}(\sum_{i=1}^{n}w_i^{Y_{n+1}}\delta_{Z_i}; 1-\alpha)
  \]}
  Now. we write $Z_i = (X_i, Y_i),$ and
  \[
  w_i = w_i^{Y_{n+1}} = \frac{\frac{dQ}{dP}(Z_i)} {\sum_{j=1}^{n+1} \frac{dQ}{dP}(Z_j)}, \quad i \in [n+1]
  \]

  Then,
  \[
  \begin{aligned}
      \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) &= \mathbb{P} \left( S_{n+1} \le \text{Quantile} \left( \sum_{i=1}^{n} w_i \delta_{S_i};\, 1-\alpha \right)\right) \\
      &= \mathbb{E} \left[ \mathbb{P} \left(S_{n+1} \le \text{Quantile} \left( \sum_{i=1}^{n} w_i \delta_{S_i};\, 1-\alpha \right) \middle| \hat{P}_{n+1} \right) \right]
  \end{aligned}
  \]
  
\end{frame}

% 20 page
\begin{frame}{Marginal coverage guarantee with General shift}
    proof of Thm 7.5) (continue) \\
    Since the score $s$ is symmetric, $s(\cdot, \mathcal{D}_{n+1}) = s(\cdot, \hat{P}_{n+1}).$ 

    Then, for $S_{n+1} = s(Z_{n+1}, \hat{P}_{n+1}),$
    \[
    \begin{aligned}
        &\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \\
        &= \mathbb{E} \left[ \mathbb{P} \left(s(Z_{n+1}, \hat{P}_{n+1}) \le \text{Quantile} \left( \sum_{i=1}^{n} w_i \delta_{s(Z_{n+1}, \hat{P}_{n+1})};\, 1-\alpha \right) \middle| \hat{P}_{n+1} \right) \right] \\
        &\overset{?}{=} \mathbb{E} \left[ \sum_{j=1}^{n+1} w_j 1 \left\{ s(Z_j; \widehat{P}_{n+1}) \le \text{Quantile} \left( \sum_{i=1}^{n} w_i \delta_{s(Z_i; \widehat{P}_{n+1})}; 1-\alpha \right) \right\} \right]
    \end{aligned}
    \]
\end{frame}

% 21 page
\begin{frame}{Marginal coverage guarantee with General shift}
    proof of Thm 7.5) (continue) \\
    Why?
    \[
    \begin{aligned}
        &\text{Quantile} \left( \sum_{i=1}^{n} w_i \delta_{s(Z_i; \hat{P}_{n+1})}; 1-\alpha \right) \in \hat{P}_{n+1} \quad (\text{by prop. 7.6.}) \\
        \Rightarrow &\left\{s(Z_{n+1}, \hat{P}_{n+1}) \le \text{Quantile} \left( \sum_{i=1}^{n} w_i \delta_{s(Z_{n+1}, \hat{P}_{n+1})};\, 1-\alpha \right) \right\} = f(Z_{n+1}) \\
        & \text{ for some }f, \text{ given }\hat{P}_{n+1} \\
        \Rightarrow &f(Z_{n+1}) \mid \hat{P}_{n+1} \overset{d}{=} \sum_{j=1}^{n+1} w_j \delta_{f(Z_j)} \quad (\text{by prop. 7.6., again})
    \end{aligned}
    \]
\end{frame}

% 22 page
\begin{frame}{Marginal coverage guarantee with General shift}
    proof of Thm 7.5) (continue) \\
    \begin{alertblock}{(Recall) Fact 2.12: Quantiles and CDFs}
    For $z \in \mathbb{R}^n,$ TFAE:
    \begin{enumerate}
        \item[(i)] $\hat{F}_z(v) = \sup\{\tau : \text{Quantile}(z; \tau) \le v \}, \quad \forall v \in \mathbb{R}$
        \item[(iii)] $\hat{F}_z(\text{Quantile}(z; \tau)) \ge \tau, \quad \forall \tau \in [0,1]$
    \end{enumerate}
    \end{alertblock}
    \begin{alertblock}{\hypertarget{7.7}{Fact 7.7}: Quantiles and CDFs}
      For $z \in \mathbb{R}^n,$ and the weights $w_1. \ldots, w_n,$
      \[
      \sum_{i=1}^{n} w_i  1 \left\{ z_i \le \text{Quantile} \left(\sum_{j=1}^{n} w_j \delta_{z_j}; \tau \right) \right\} \ge \tau \quad \forall \tau \in [0,1]
      \]
    \end{alertblock}
    Hence,
    \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \ge \mathbb{E}[1 - \alpha] = 1 - \alpha \qed
    \] 
\end{frame}

% 23 page
\begin{frame}{Marginal coverage guarantee with General shift}
    proof of Prop 7.6) 
    For all meas'l sets $A,$ and $B$,
    {\small \[
    \begin{aligned}
    & \mathbb{P}_{(Z_1,\ldots,Z_{n+1})\sim P^n \times Q} (Z_{n+1} \in A, \hat{P}_{n+1} \in B) \\
    \; = &\; \mathbb{E}_{P^n \times Q} \big[ \mathbf{1}\{Z_{n+1} \in A\} \mathbf{1}\{\hat{P}_{n+1} \in B\} \big] \\
    \; = &\; \mathbb{E}_{P^{n+1}} \left[ \frac{dQ}{dP}(Z_{n+1}) \cdot \mathbf{1}\{Z_{n+1} \in A\} \mathbf{1}\{\hat{P}_{n+1} \in B\} \right] \\
    \; = &\; \mathbb{E}_{P^{n+1}} \left[ w_{n+1}\sum_{i=1}^{n+1}\frac{dQ}{dP}(Z_i) \cdot \mathbf{1}\{Z_{n+1} \in A\} \mathbf{1}\{\hat{P}_{n+1} \in B\} \right] \\
    \; = &\; \sum_{i=1}^{n+1} \mathbb{E}_{P^{n+1}} \left[ w_{i} \frac{dQ}{dP}(Z_{n+1}) \cdot \mathbf{1}\{Z_{i} \in A\} \mathbf{1}\{\hat{P}_{n+1} \in B\} \right] \\
    \; = &\; \sum_{i=1}^{n+1} \mathbb{E}_{P^{n} \times Q} \left[ w_{i} \mathbf{1}\{Z_{i} \in A\} \mathbf{1}\{\hat{P}_{n+1} \in B\} \right]
    \end{aligned}
    \]}
    Hence,
    \[
    Z_{n+1}\mid \hat{P}_{n+1} \sim \sum_{i=1}^{n+1} w_i \delta_{Z_i} \quad \qed
    \] 
\end{frame}

% 24 page
\begin{frame}{Comparing distribution shift and conditional coverage}
   
   {\small We have a difficulty about the conditional coverage:
   \begin{itemize}
       \item If it guarantees the test-conditional coverage, i.e.,
       \[
       \mathbb{P}(Y_{n+1}\in \mathcal{C}(X_{n+1}) \mid X_{n+1}=x)\ge 1-\alpha \quad \forall x ,
       \]
       it implies coverage for any covariate shift. (i.e. any distribution of $X$, $Q_X$)
       \item If it guarantees the label-conditional coverage, i.e.,
       \[
       \mathbb{P}(Y_{n+1}\in \mathcal{C}(X_{n+1}) \mid Y_{n+1}=y)\ge 1-\alpha \quad \forall y ,
       \]
       it implies coverage for any label shift. (i.e. any distribution of $Y$, $Q_Y$)
       \vspace{1.0em}
       \item But it is harder, since $\mathcal{C}(X)$ will be very large unless we have a large number of obs. in each category. (ex. $\sum_{i\in [n]} \mathbf{1}\{Y_i=y\}$ is large, for each $y \in \mathcal{Y}$)
       \item Hence, we guarantee the marginal coverage relative to a particular distribution, (ex. $P_{X \mid Y} \times Q_Y$) which is more informative.
   \end{itemize}}
\end{frame}

% 25 page
% 26 page
\section{Localized conformal prediction}
\begin{frame}{Improving the test-conditional coverage}
  
  {\small \begin{itemize}
     \item (Ch4) It's impossible in general to ensure test-conditional coverage.
     \item (Ch5) Certain score functions lead to approximate conditional coverage under additional assumptions.
     \vspace{1.0em}
     \item Consider the target
     \[
     \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1}) \mid X_{n+1}) \geq 1 - \alpha
     \]
     \item The prediction set is determined by the conformal score of all training points, including the data points which are far away from the test point $X_{n+1}$.
     \item If the distribution of scores is very different across different regions of $\mathcal{X}$?
     \begin{figure}
         \centering
         \includegraphics[width=0.25\linewidth]{image7.2.png}
         \label{fig:placeholder}
     \end{figure}
  \end{itemize}}

\end{frame}

% 27 page
\begin{frame}{The distance between two feature vectors}

  \begin{itemize}
      \item Hence, given $X_{n+1}$, we can think of the strategy that gives more weight to score calculations when data point is closer to $X_{n+1}.$
      \begin{block}{Definition : Localization Kernel}
        The \textbf{localization kernel} is the function $H: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}_{+}$. \\
        For example, $H(x, x') = \exp\{-\frac{\|x - x'\|_2^2}{2h^2}\} \quad \text{ for } x, x' \in \mathbb{R}^d, h > 0$
      \end{block}
      \item Let the weight $w_i$ on the data point $(X_i, Y_i)$ be
      \[
      w_i \propto H(X_i, X_{n+1}),
      \]
      and go back to \hyperlink{7.1}{Algorithm 7.1}.
      \item[$\Rightarrow$] $\cancel{\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1}) \mid X_{n+1}) \geq 1 - \alpha}$,  and also
      \item[ ] $\cancel{\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \geq 1 - \alpha}$
  \end{itemize}
  
\end{frame}

% 28 page
\begin{frame}{Localized Conformal Algorithm}
    \begin{exampleblock}{Algorithm 7.8: Localized CP}
    \begin{enumerate}
      \item Input: Training data $(X_1,Y_1), \ldots, (X_n,Y_n)$, test pt. $X_{n+1}$, target coverage level $1-\alpha$, conformal score ftn. $s$, and localization kernel $H$
      \item For each possible response value $y \in \mathcal{Y},$ 다음을\ 계산
        \begin{enumerate}
            \item 각 sample 별 score $S_i^y = s((X_i, Y_i); \mathcal{D}_{n+1}^y), \quad i = 1, \ldots, n.$
            \item Test point score $S_{n+1}^y = s((X_{n+1}, y); \mathcal{D}_{n+1}^y)$ 
            \item Weight $w_{i,j} = \frac{H(X_j, X_i)}{\sum_{j'=1}^{n+1} H(X_{j'}, X_i)}, \quad i,j = 1, \ldots, n+1.$
            \item Weighted score \textcolor{red}{$\tilde{S}_i^y = \sum_{j=1}^{n+1} w_{i,j} \mathbf{1}\{S_j^y < S_i^y\} \quad i = 1, \ldots, n+1.$} 
            \item Quantile $\tilde{q}^y = \text{Quantile}(\textcolor{red}{\tilde{S}_1^y, \ldots, \tilde{S}_{n+1}^y} ; 1-\alpha)$
        \end{enumerate}
      \item Return the prediction set $\mathcal{C}(X_{n+1}) = \{ y \in \mathcal{Y} : \tilde{S}_{n+1}^y \leq \tilde{q}^y \}$
    \end{enumerate}
  \end{exampleblock}
  {\small This method is exactly same as the intuitive approach with Algorithm 7.1, but we use the data-dependent threshold $\tilde{q}^y$, rather than the usual threshold $1-\alpha.$}
\end{frame}


% 29 page
\begin{frame}{Localized Conformal Algorithm}
  \begin{alertblock}{Proposition 7.9: Localized CP is a version of Full CP}
    The prediction set defined in Algorithm 7.8 is equivalent to the full conformal prediction set, using the score function as
    \[
    \tilde{s}((x, y); \mathcal{D}) = \sum_{j=1}^{m} \frac{H(x_j, x)}{\sum_{j'=1}^{m} H(x_{j'}, x)} \mathbf{1}\{ s(x_j, y_j; \mathcal{D}) < s(x, y; \mathcal{D}) \}
    \]
    for any $(x,y)$ and any dataset $\mathcal{D} = ((x_1, y_1), \ldots, (x_m, y_m)).$
  \end{alertblock}
  Also, since $s$ and $\tilde{s}$ are symmetric in $\mathcal{D}$, by Thm 3.2, the \textbf{marginal coverage} must hold for localized CP, under the exchangeability assumption.
\end{frame}

% 30 page
\begin{frame}{Approximate conditional coverage}
    \begin{itemize}
        \item Now, we try to show the Localized CP can guarantee the approximate conditional coverage.
        \item Before that, we restrict the definition of the localization kernel for our algorithm.
    \end{itemize}
    \begin{block}{Definition : Localization Kernel with Density}
        The \textbf{localization kernel} is the function $H: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}_{+}$, such that $H(x,\cdot)$ is a density w.r.t. some measure $\nu$ for each $x \in \mathcal{X}$, i.e.,
        \[
        \int_{\mathcal{X}} H(x, x')\, d\nu(x') = 1
        \]
        For example, $H(x, x') = \exp\{-\frac{\|x - x'\|_2^2}{2h^2}\} \quad \text{ for } x, x' \in \mathbb{R}^d, h > 0$
    \end{block}
\end{frame}

% 31 page
\begin{frame}{Randomly-localized Conformal Algorithm}
    \begin{exampleblock}{Algorithm 7.10: Randomly-localized CP}
    \begin{enumerate}
      \item Input: Training data $(X_1,Y_1), \ldots, (X_n,Y_n)$, test pt. $X_{n+1}$, target coverage level $1-\alpha$, conformal score ftn. $s$, and localization kernel $H$
      \item \textcolor{red}{Sample $\tilde{X}_{n+1} \sim H(X_{n+1}, \cdot)$}
      \item For each possible response value $y \in \mathcal{Y},$ 다음을\ 계산
        \begin{enumerate}
            \item 각 sample 별 score $S_i^y = s((X_i, Y_i); \mathcal{D}_{n+1}^y), \quad i = 1, \ldots, n.$
            \item Test point score $S_{n+1}^y = s((X_{n+1}, y); \mathcal{D}_{n+1}^y)$ 
            \item \textcolor{red}{Weight $w_i = \frac{H(X_i, \tilde{X}_{n+1})}{\sum_{j=1}^{n+1} H(X_j, \tilde{X}_{n+1})}, \quad i,j = 1, \ldots, n+1.$}
            \item Quantile $\hat{q}^y = \text{Quantile}(\sum_{i=1}^{n+1}w_i\delta_{S_i^y} ; 1-\alpha)$
        \end{enumerate}
      \item Return the prediction set $\mathcal{C}(X_{n+1}) = \{ y \in \mathcal{Y} : S_{n+1}^y \leq \hat{q}^y \}$
    \end{enumerate}
  \end{exampleblock}
\end{frame}

% 32 page
\begin{frame}{Conditional coverage guarantee with Localized CP}
  \begin{alertblock}{{\scriptsize Thm 7.11: (Approximate) conditional coverage guarantee with Randomly-localized CP}}
    {\small For $(X_1, Y_1), \ldots,(X_{n+1}, Y_{n+1}) \overset{\text{i.i.d.}}{\sim} P$, for some $P$ and $s$ is a symmetric score ftn. \\
    Let $\mathcal{C}(X_{n+1})$ be the output of Algorithm 7.10., then
    \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1}) \mid \tilde{X}_{n+1}) \geq 1 - \alpha \quad \text{a.s.}
    \]}
  \end{alertblock}
  proof) Note $(X_1, Y_1), \ldots,(X_{n}, Y_{n}) \;\perp\!\!\!\perp\; (X_{n+1}, Y_{n+1}, \tilde{X}_{n+1}). $ , and
  \[
  \begin{aligned}
      X_{n+1} &\sim P_X, \\  
      Y_{n+1} \mid X_{n+1} &\sim P_{Y|X}, \\  
      \tilde{X}_{n+1} \mid (X_{n+1}, Y_{n+1}) &\sim H(X_{n+1}, \cdot)
  \end{aligned}
  \]
  Then,
  \[
  \begin{aligned}
      &(X_{n+1}, Y_{n+1}) \mid \tilde{X}_{n+1} \sim (P_X \circ H(\cdot, \tilde{X}_{n+1})) \times P_{Y|X} \\
      &\text{where } \frac{d(P_X \circ H(\cdot, \tilde{X}_{n+1}))(x)}{dP_X(x)} \propto H(x, \tilde{X}_{n+1})
  \end{aligned}
  \]
\end{frame}

% 33 page
\begin{frame}{Conditional coverage guarantee with Localized CP}
    proof) (continue) \\
    Hence, given $\tilde{X}_{n+1}$, this is an instance of covariate shift-the distribution of $X_{n+1}$ is no longer $P_X$, but the conditional distribution $P_{Y\mid X}$ is invariant. \\
    \vspace{1.0em}
    We can apply Algorithm 7.1 (Weight full CP) to this, and the weight is
    \[
    \tilde{w}_j \propto H(X_j, \tilde{X}_{n+1}) \propto \frac{d(P_X \circ H(\cdot, \tilde{X}_{n+1}))(X_j)}{dP_X(X_j)}.
    \] 
    Finally, above algorithm equals to Algorithm 7.10. \qed
\end{frame}

% 34 page
\begin{frame}{Cf. Back to 4.7.2.}
    Original Test-coverage:
    \[
    \begin{aligned}
        &\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1}) \mid X_{n+1} \in \mathcal{X}_0) \ge 1 - \alpha \\ 
        &\text{for all } P \text{ and all } \mathcal{X}_0 \subseteq \mathcal{X}, \text{ with } P_X(\mathcal{X}_0) \ge \delta
    \end{aligned}
    \]
    Relaxed version: (only certain special subsets $\mathcal{X}_0$)
    \[
    \begin{aligned}
        &\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1}) \mid \|X_{n+1} - x_*\| \le r_*) \ge 1 - \alpha \\
        &\text{for all } P \text{ and all } x_* \in \mathcal{X}, r_* \ge 0 \text{ with } P_X(\|X - x_*\| \le r_*) \ge \delta
    \end{aligned}
    \]
    It is same as only certain special kernel $H(\cdot, \cdot)$:
    \[
    \begin{aligned}
        &\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1}) \mid x_*) \geq 1 - \alpha \\
        &\text{if } x_* \sim H(X_{n+1}, \cdot) \quad \text{with } \int_{B{(x,r_*)}}H(x, x')d\nu(x') \ge \delta
    \end{aligned}
    \]
\end{frame}

% 32 page
% 33 page
\section{Fixed-weight conformal prediction}
\begin{frame}{Robustness for non-exchangeability}
    Example: Time series
    \begin{itemize}
        \item For time points $1, \ldots, {n+1},$ if we want to get a prediction set of $X_{n+1},$
        \item we expect that the distribution of recent points is closer to that of the test point.
        \item[$\Rightarrow$] Set recent data higher weights relative to data from long ago.
        \vspace{4.0em}
        \item In this chapter, consider the fixed weights, i.e., data-independent weights,
        \item for a conservative approach with certain violations of exchangeability.
    \end{itemize}
\end{frame}

% 34 page
\begin{frame}{Fixed-weight CP}
  \begin{alertblock}{{\small Thm 7.12: Lower bound of marginal coverage with fixed-weight CP}}
    {\small For the weights $w_1, \ldots, w_{n+1}$ with $w_{n+1} \ge w_i, \; \forall i \in [n],$ r.v.s $Z_1, \ldots Z_{n+1}$ with any joint distribution, and the symmetric score $s,$ let
    \[
    \mathcal{C}(X_{n+1}) = \{ y : S_{n+1}^y \leq \hat{q}_w \}, \quad \text{where } \hat{q}_w^y = \text{Quantile}\left(\sum_{i=1}^{n+1} w_i\delta_{S_i^y} ; 1-\alpha\right)
    \]
    Then,} 
    {\scriptsize \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \ge 1 - \alpha - \sum_{i=1}^n w_i \, d_{\text{TV}}\big((Z_1,\dots,Z_{n+1}), (Z_1,\dots,Z_{i-1},Z_{n+1},Z_{i+1},\dots,Z_i)\big)
    \]}
    {\small where $ d_{\text{TV}}$ means the total variation distance between distributions.}
  \end{alertblock}

  \begin{itemize}
      {\footnotesize \item Exchangeability Assumption X
      \item[$\Rightarrow$] Instead, introduce the TV distance of the swapped versions of the data.}
      \begin{enumerate}
        {\footnotesize \item If exchangeable, $d_{\text{TV}} =0.$
        \item If not, set a smaller weight $w_i$ when the distribution of $Z_i$ is more different from that of $Z_{n+1}.$}
      \end{enumerate}
  \end{itemize}
\end{frame}

% 35 page
\begin{frame}{Fixed-weight CP}
    proof) For the vector of whole scores $S=(S_1, \ldots,S_{n+1})$, and swapped scores $S^i = (S_1, \ldots, S_{i-1}, S_{n+1}, S_{i+1}, \ldots, S_n, S_i),$ \\
    \vspace{0.5em}
    define a \textbf{score vector returning operator} $h:(\mathcal{X}\times\mathcal{Y})^{n+1}\rightarrow\mathbb{R}^{n+1}$ as 
    \[
    h(z_1, \ldots, z_{n+1}) = (s(z_1; (z_1, \ldots, z_{n+1})), \ldots, s(z_{n+1}; (z_1, \ldots, z_{n+1}))),
    \]
    i.e., $S=h(Z_1, \ldots, Z_{n+1}),$ and \[
    S^i = h(Z_1, \ldots, Z_{i-1}, Z_{n+1}, Z_{i+1}, \ldots, Z_n, Z_i) \quad (\because s \text{ is symmetric.})
    \]
\end{frame}

% 36 page
\begin{frame}{Fixed-weight CP}
    proof) (continue) \\
    Then, by Data Processing Inequality,
    \[
    \begin{aligned}
        d_{\text{TV}}(S, S^i) &= d_{\text{TV}}(h(Z_1, \ldots, Z_{n+1}), h(Z_1, \ldots, Z_{i-1}, Z_{n+1}, Z_{i+1}, \ldots, Z_n, Z_i)) \\
        &\le d_{\text{TV}}((Z_1, \ldots, Z_{n+1}), (Z_1, \ldots, Z_{i-1}, Z_{n+1}, Z_{i+1}, \ldots, Z_n, Z_i))
    \end{aligned}
    \]
    and it suffices to show
    \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \ge 1 - \alpha - \sum_{i=1}^n w_i \cdot d_{\text{TV}}(S, S^i).
    \vspace{1.0em}
    \]
    {\small (Important Fact) We use $d_{\text{TV}}(S, S^i)$, rather than $d_{\text{TV}}((\mathcal{D}_n, Z_{n+1}), (\mathcal{D}_n^{Z_{n+1}}, Z_i)).$ 
    
    $\Rightarrow$ It is working better when the total variation distance between the data points $Z_i$ and $Z_{n+1}$ could be large, but the distributions of the corresponding scores $S_i$, $S_{n+1}$ might be similar.}
\end{frame}

% 37 page
\begin{frame}{Fixed-weight CP}
    proof) (continue) \\
    {\small Recall the definition of Total Variation, for any measurable $A \subseteq (\mathcal{X}\times\mathcal{Y})^{n+1},$
    \[
    d_{\text{TV}}(S, S^i) = \sup_A \|p_S (A) - p_{S'}(A)\|
    \]
    Using it, for $S = S^{n+1},$}
    {\footnotesize \[
    \begin{aligned}
        \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) = \mathbb{P}\left( S_{n+1} \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{S_j}; 1 - \alpha \right)\right)& \\
        = \sum_{i=1}^{n+1} w_i \cdot \mathbb{P}\left(S_{n+1} \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{S_j}; 1 - \alpha \right)\right)& \\
        \ge \sum_{i=1}^{n+1} w_i \left[ \mathbb{P}\left((S^i)_{n+1} \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{(S^i)_j}; 1 - \alpha \right)\right)& - d_{\text{TV}}(S, S^i)\right] \\
        = \mathbb{E} \left[\sum_{i=1}^{n+1} w_i \cdot \mathbf{1}\left\{ (S^i)_{n+1} \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{(S^i)_j}; 1 - \alpha \right)\right\}\right]& - \sum_{i=1}^n w_i \, d_{\text{TV}}(S, S^i).
    \end{aligned}
    \]}
\end{frame}

% 38 page
\begin{frame}{Fixed-weight CP}
    proof) (continue) \\
    Claim: 
    {\scriptsize \[
    S_i \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{S_j}; 1 - \alpha\right) \Rightarrow (S^i)_{n+1} \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{(S^i)_j}; 1 - \alpha\right) \tag{7.8}
    \]}
    If holds,
    \[
    \begin{aligned}
        &\mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \\
        \ge \; &\mathbb{E}\left[\sum_{i=1}^{n+1} w_i \cdot \mathbf{1}\left\{ S_i \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{S_j}; 1 - \alpha\right) \right\}\right] - \sum_{i=1}^n w_i \, d_{\text{TV}}(S, S^i) \\
        \ge \; &1 - \alpha - \sum_{i=1}^n w_i \cdot d_{\text{TV}}(S, S^i). \quad (\because \text{\hyperlink{7.7}{Fact. 7.7}})
    \end{aligned}
    \]
\end{frame}

% 39 page
\begin{frame}{Fixed-weight CP}
    proof) (continue) \\
    To show (7.8), we only consider the cases $i \in [n].$ We will need following result: 
    \begin{alertblock}{(Recall) Lemma 3.4: Replacement Lemma}
        Let $v_1, \ldots, v_{n+1} \in \mathbb{R}.$ For any $t \in [0,1],$
        {\small \[
        v_{n+1} \leq \text{Quantile}(v_1, \ldots, v_n; t) \;\;\Longleftrightarrow\;\; v_{n+1} \leq \text{Quantile}(v_1, \ldots, v_n, v_{n+1}; t(1+\frac{1}{n}))
        \]}
    \end{alertblock}
    \begin{alertblock} {Lemma 7.13: Replacement Lemma Generalization}
        For any distributions $P_0$, and $P_1$, and some $\epsilon  \in [0,1],$ let $P=(1-\epsilon)P_0 + \epsilon P_1$ be their mixture.\\
        Then for any $x \in \mathbb{R}$, and $\tau \in [0,1],$
        \[
        x \le \text{Quantile}(P; \tau) \;\Rightarrow\; x \le \text{Quantile}\big((1 - \epsilon) \cdot P_0 + \epsilon \cdot \delta_x; \tau\big)
        \]
    \end{alertblock}
\end{frame}

% 40 page
\begin{frame}{Fixed-weight CP}
    proof) (continue) \\
    For applying this, we setup $\epsilon = w_{n+1} - w_i \ge 0, \tau = 1 - \alpha,$ and
    {\small \[
    P_1 = \delta_{S_{n+1}}, \quad P_0 = \frac{\sum_{j \in [n],\, j \ne i} w_j \cdot \delta_{S_j} + w_i \cdot (\delta_{S_i} + \delta_{S_{n+1}})}{1 - \epsilon}
    \]}

    Then,
    {\small \[
    \begin{aligned}
        &S_i \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{S_j}; 1 - \alpha\right) \\
        \Leftrightarrow \; &S_i \le \text{Quantile}\left((1-\epsilon)P_0 + \epsilon P_1; 1 - \alpha\right) \\
        \Rightarrow \; &S_i \le \text{Quantile}\left((1-\epsilon)P_0 + \epsilon \delta_{S_i}; 1 - \alpha\right) \\
        \Leftrightarrow \; &S_i \le \text{Quantile}\left(\sum_{j \in [n], j \ne i} w_j \cdot \delta_{S_j} + w_{n+1} \cdot \delta_{S_i} + w_i \cdot \delta_{S_{n+1}}; 1 - \alpha\right) \\
        \Leftrightarrow \; &(S^i)_{n+1} \le \text{Quantile}\left(\sum_{j=1}^{n+1} w_j \delta_{(S^i)_j}; 1 - \alpha\right). \qed
    \end{aligned}
    \]}
\end{frame}

% 41 page
% 42 page
\section{A general outlook through weighted permutations}
\begin{frame}{Generalized CP}
   \begin{itemize}
       \item In this chapter, we talk about reinterpretation CP through weighted distributions over permutations.
       \item We can generalize CP to work with any joint dist.s, even non-exchangeable.
       \vspace{2.0em}
       \item The main idea is if we know only \textbf{the probability of observing every ordering of the data points conditionally on their values}, this is enough to run CP. 
   \end{itemize}
\end{frame}

% 43 page
\begin{frame}{Generalized CP}
  \begin{alertblock}{(Recall) Proposition 7.6: Conditioning on the empirical distribution}
    {\small Let $Z_1, \ldots, Z_n \overset{\text{i.i.d.}}{\sim} P$ and $Z_{n+1} \sim Q$ independently, then
    \[
    Z_{n+1}\mid \hat{P}_{n+1} \sim \sum_{i=1}^{n+1} w_i \delta_{Z_i}, \text{ where } w_i = \frac{\frac{dQ}{dP}(Z_i)} {\sum_{j=1}^{n+1} \frac{dQ}{dP}(Z_j)}, \quad i \in [n+1]
    \]}
  \end{alertblock}
  Now, let the density $f:(\mathcal{X}\times\mathcal{Y})^{n+1} \rightarrow \mathbb{R}$ as $(Z_1, \ldots, Z_{n+1}) \sim f,$ then
  \[
  Z_{n+1} \mid \hat{P}_{n+1} \sim \frac{ \sum_{\sigma \in \mathcal{S}_{n+1}} f(Z_{\sigma(1)}, \ldots, Z_{\sigma(n+1)}) \, \delta_{Z_{\sigma(n+1)}}}{\sum_{\sigma \in \mathcal{S}_{n+1}} f(Z_{\sigma(1)}, \ldots, Z_{\sigma(n+1)})}. \quad (\because \text{non-i.i.d.})
  \]
  That is, once we fix the values of the data points, then we can explicitly describe the distribution of the test point in terms of their ordering, if we have knowledge of the joint density $f$.
\end{frame}

% 44 page
\begin{frame}{Generalized CP}
  \begin{alertblock}{Thm 7.14: Generalized Weighted CP}
    {\small For $(Z_1, \ldots Z_n, Z_{n+1}) \sim P$, and the joint pdf $f$ for any measure on $\mathcal{X} \times \mathcal{Y}$, fix any score $s$ (not necessarily symmetric), and define the prediction set}
    {\footnotesize \[
    \begin{aligned}
        \mathcal{C}(X_{n+1})& = \{ y : s((X_{n+1}, y); \mathcal{D}_{n+1}) \le \hat{q}_{w}^{y} \}, \\ \text{where } \hat{q}_{w}^{y} = &\text{Quantile}\left(\sum_{\sigma \in \mathcal{S}_{n+1}} w_{\sigma}^{y} \, \delta_{s(Z_{\sigma(n+1)}^{y}; \mathcal{D}_{n+1}^{y, \sigma})};1 - \alpha\right)
    \end{aligned}
    \]}
    {\small where}
    {\footnotesize \[
    w_{\sigma}^{y} = \frac{ f(Z_{\sigma(1)}^{y}, \ldots, Z_{\sigma(n+1)}^{y})}{\sum_{\sigma' \in \mathfrak{S}_{n+1}} f(Z_{\sigma'(1)}^{y}, \ldots, Z_{\sigma'(n+1)}^{y})}
    \]}
    {\small and the permuted dataset}
    {\footnotesize \[
    (D_{n+1}^y)_\sigma = (Z_{\sigma(1)}^y, \ldots, Z_{\sigma(n+1)}^y) = ((X_{\sigma(1)}, Y_{\sigma(1)}), \ldots, (X_{\sigma(n+1)}, y))
    \]}
    {\small Then, it satisfies the marginal coverage guarantee,}
    {\footnotesize \[
    \mathbb{P}(Y_{n+1} \in \mathcal{C}(X_{n+1})) \geq 1 - \alpha
    \]}
  \end{alertblock}
\end{frame}

% 45 page
\begin{frame}{Generalized CP}
    \begin{itemize}
        \item Pros
        \begin{enumerate}
            \item $f$를\ 잘\ 몰라도\ 가능한\ 경우\ 존재: i.i.d. case $\left(w_\sigma^y = \frac{1}{(n+1)!} \right)$, etc.
            \item 가중치가 entire permutation $\sigma$에\ 의존\ (more general than covariate/label shift)
            \item nonexchangeable data, nonsymmetric algorithm에\ 적용\ 가능
        \end{enumerate}
        \item Cons
        \begin{enumerate}
            \item 대부분\ $f$를\ 알아야\ 함
            \item $(n+1)!$의\ permutation을\ 모두\ 계산
        \end{enumerate}
        \item Worth
        \begin{enumerate}
            \item Unified version of existing method of Conformal Prediction
            \item It points toward an underlying structure that facilitates conformal prediction.
        \end{enumerate}
    \end{itemize}
\end{frame}

\end{document}